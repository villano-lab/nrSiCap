{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R68 Yield Fitting\n",
    "This notebook is meant to bring together all the R68 simulations and measurements in order to extract an estimate of the NR yield function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../mplstyles\n",
      "3.0.3\n",
      "/home/phys/villaa/analysis/misc/R68_paper2019/mplstyles\n"
     ]
    }
   ],
   "source": [
    "#we may need some code in the ../python directory and/or matplotlib styles\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../python/')\n",
    "\n",
    "#set up matplotlib\n",
    "os.environ['MPLCONFIGDIR'] = '../mplstyles'\n",
    "print(os.environ['MPLCONFIGDIR'])\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "#got smarter about the mpl config: see mplstyles/ directory\n",
    "plt.style.use('standard')\n",
    "print(mpl.__version__) \n",
    "print(mpl.get_configdir())\n",
    "\n",
    "\n",
    "#fonts\n",
    "# Set the font dictionaries (for plot title and axis titles)\n",
    "title_font = {'fontname':'Arial', 'size':'16', 'color':'black', 'weight':'normal',\n",
    "              'verticalalignment':'bottom'} # Bottom vertical alignment for more space\n",
    "axis_font = {'fontname':'Arial', 'size':'32'}\n",
    "legend_font = {'fontname':'Arial', 'size':'22'}\n",
    "\n",
    "#fonts global settings\n",
    "mpl.rc('font',family=legend_font['fontname'])\n",
    "\n",
    "\n",
    "#set up numpy\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measured Data\n",
    "Load in measured data for PuBe and Bkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480634,)\n",
      "(174510,)\n"
     ]
    }
   ],
   "source": [
    "fpube = open('data/r68_n125_PuBe_cgood_final_PTOFkeV_2keV_scan_fmt.txt')\n",
    "fbknd = open('data/r68_n125_bkg_cgood_final_PTOFkeV_2keV_scan_fmt.txt')\n",
    "\n",
    "d = np.asarray([x.split() for x in fpube.readlines()],dtype=np.float)\n",
    "db = np.asarray([x.split() for x in fbknd.readlines()],dtype=np.float)\n",
    "\n",
    "fpube.close()\n",
    "fbknd.close()\n",
    "\n",
    "#Measured event energies in keV for PuBe (dE) and background (dbE)\n",
    "E_PuBe = d[:,1]\n",
    "E_Bkg = db[:,1]\n",
    "print(np.shape(E_PuBe))\n",
    "print(np.shape(E_Bkg))\n",
    "\n",
    "#TODO:\n",
    "#Measured data live time estimates\n",
    "#https://zzz.physics.umn.edu/cdms/doku.php?id=cdms:k100:run_summary:run_68:run_68_rateandlivetime#iii_read_efficiency\n",
    "tlive_PuBe = 97.9*3600 #[s]\n",
    "#tlive_bkg = tlive_PuBe*193/640 # Naive scaling by number of series\n",
    "tlive_bkg = 24.1*3600 #[s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Data Skims\n",
    "\n",
    "We'll use several skims of the full simulation data with cuts to select different interactions. The sims are done with commands like\n",
    "\n",
    "```./sima2py.py --regex 'Run68_gdirect_bknd_R68_PuBe_0x0006_10M_1550\\S+.txt' --filedir /data/chocula/villaa/k100Sim_Data/captureCal/ --outfile /data/chocula/villaa/k100Sim_Data/captureCalhdf5/R68_gdirect_testskim_superhighstat_cap.h5 --cuts NRc```\n",
    "\n",
    "These skims only contain a some (2716) of all the simulated data files. The outputs are located in `/data/chocula/villaa/k100Sim_Data/captureCalhdf5/`:\n",
    "\n",
    "| File                                          | Description  |\n",
    "| :---                                          |    :---:     |\n",
    "| `R68_gdirect_testskim_superhighstat_nocap.h5` | NR, No capture   |\n",
    "| `R68_gdirect_testskim_superhighstat_nocap_er_lowe.h5` | ER, No capture   |\n",
    "\n",
    "\n",
    "Nick did skims of the full set of simulated data using the flag: ```--regex 'Run68_gdirect_bknd_R68_PuBe_0x0006_10M_155\\S+.txt'``` with the sima2py in the git hash: f0c2a47. These skims use the full set of 40261 G4 output files.\n",
    "\n",
    "The resulting skims are located in `/home/mast/cdms/analysis/run068/simulations/k100_ncapture_cal/data/`:\n",
    "\n",
    "| File                                          | Description  |\n",
    "| :---                                          |    :---:     |\n",
    "| `R68_gdirect_testskim_stupidhighstat_nocap_nr.h5` | NR, No capture   |\n",
    "| `R68_gdirect_testskim_stupidighstat_nocap_er_lowe.h5` | ER, No capture   |\n",
    "\n",
    "Some of the 40261 G4 output files are incomplete (missing lines, values, etc.) These are simply skipped in the skim. There are 932 such files as can be seen by counting the 'Incomplete' labels in the output of sima2py, such as in the file ```R68_gdirect_testskim_stupidhighstat_nocap_er_lowe_skimfiles.txt```. This leaves 39329 files which were actually included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(528848, 7)\n",
      "(129555, 7)\n"
     ]
    }
   ],
   "source": [
    "#===============to suppress h5py warning see:\n",
    "#https://github.com/h5py/h5py/issues/961\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import h5py\n",
    "warnings.resetwarnings()\n",
    "\n",
    "#f_nr_nocap = h5py.File(\"/data/chocula/villaa/k100Sim_Data/captureCalhdf5/R68_gdirect_testskim_superhighstat_nocap.h5\",\"r\")\n",
    "f_nr_nocap = h5py.File(\"data/R68_gdirect_testskim_stupidhighstat_nocap_nr.h5\",\"r\")\n",
    "data_nr_nocap = f_nr_nocap['geant4/hits']\n",
    "\n",
    "#f_er_nocap = h5py.File(\"/data/chocula/villaa/k100Sim_Data/captureCalhdf5/R68_gdirect_testskim_superhighstat_nocap_er_lowe.h5\",\"r\")\n",
    "f_er_nocap = h5py.File(\"data/R68_gdirect_testskim_stupidhighstat_nocap_er_lowe.h5\",\"r\")\n",
    "data_er_nocap = f_er_nocap['geant4/hits']\n",
    "\n",
    "print(np.shape(data_nr_nocap))\n",
    "print(np.shape(data_er_nocap))\n",
    "\n",
    "#https://zzz.physics.umn.edu/cdms/doku.php?id=cdms:k100:run_summary:run_68:run_68_n125:full_signal_fit&#efficiencies_and_live_time\n",
    "tlive_g4 = 18.9*3600 #s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now make a dataframe with the restricted data\n",
    "#Columns are:\n",
    "#cols=['EV', 'DT', 'TS', 'P', 'Type', 'E1', 'D3', 'PX3', 'PY3', 'PZ3', 'X3', 'Y3', 'Z3',\n",
    "#      'time3', 'PX1', 'PY1', 'PZ1', 'X1', 'Y1', 'Z1', 'time1', 'nCap']\n",
    "\n",
    "cols=['EV', 'D3', 'X3', 'Y3', 'Z3','time3', 'nCap']\n",
    "\n",
    "sel_names=['EV', 'D3', 'X3', 'Y3', 'Z3', 'time3', 'nCap']#Select these variables\n",
    "sel=[cols.index(i) for i in sel_names]\n",
    "\n",
    "nr_nocap_data = data_nr_nocap[:,sel]\n",
    "nr_nocap_dataframe = pd.DataFrame(data=nr_nocap_data, columns=sel_names)\n",
    "er_nocap_data = data_er_nocap[:,sel]\n",
    "er_nocap_dataframe = pd.DataFrame(data=er_nocap_data, columns=sel_names)\n",
    "\n",
    "#need unique event numbers in case of (rare) duplicate 'EV's\n",
    "nr_nocap_evnew=np.cumsum(np.diff(nr_nocap_data[:,0],prepend=nr_nocap_data[0,0]).astype(bool).astype(float))\n",
    "nr_nocap_dataframe.insert(0,'EVnew',nr_nocap_evnew)\n",
    "\n",
    "er_nocap_evnew=np.cumsum(np.diff(er_nocap_data[:,0],prepend=er_nocap_data[0,0]).astype(bool).astype(float))\n",
    "er_nocap_dataframe.insert(0,'EVnew',er_nocap_evnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Group hits into vectors for each thrown event\n",
    "#These loops can take a while\n",
    "\n",
    "groupbyvec=['EVnew']\n",
    "\n",
    "#NR, no capture\n",
    "max_vec_nr_nocap = np.max(nr_nocap_dataframe.groupby(groupbyvec,axis=0).size())\n",
    "vec0_nr_nocap = np.zeros((1,max_vec_nr_nocap))\n",
    "\n",
    "evec_nr_nocap = np.zeros((0,max_vec_nr_nocap))#Hit energies\n",
    "nhit_nr_nocap = np.zeros((0,1))#Number of hits\n",
    "\n",
    "nr_nocap_grouped=nr_nocap_dataframe.groupby(groupbyvec).agg({'D3':list})\n",
    "for d3 in nr_nocap_grouped.D3:\n",
    "    d3i=vec0_nr_nocap.copy()\n",
    "    d3i[0,0:np.shape(d3)[0]] = np.transpose(np.asarray(d3))\n",
    "    evec_nr_nocap = np.append(evec_nr_nocap,d3i*1e6,0) #convert from MeV to eV\n",
    "    nhit_nr_nocap = np.append(nhit_nr_nocap,np.shape(d3)[0])\n",
    "    \n",
    "#ER, no capture\n",
    "max_vec_er_nocap = np.max(er_nocap_dataframe.groupby(groupbyvec,axis=0).size())\n",
    "vec0_er_nocap = np.zeros((1,max_vec_er_nocap))\n",
    "\n",
    "evec_er_nocap = np.zeros((0,max_vec_er_nocap))#Hit energies\n",
    "nhit_er_nocap = np.zeros((0,1))#Number of hits\n",
    "\n",
    "er_nocap_grouped=er_nocap_dataframe.groupby(groupbyvec).agg({'D3':list})\n",
    "for d3 in er_nocap_grouped.D3:\n",
    "    d3i=vec0_er_nocap.copy()\n",
    "    d3i[0,0:np.shape(d3)[0]] = np.transpose(np.asarray(d3))\n",
    "    evec_er_nocap = np.append(evec_er_nocap,d3i*1e6,0) #convert from MeV to eV\n",
    "    nhit_er_nocap = np.append(nhit_er_nocap,np.shape(d3)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated Capture Data\n",
    "(Text and code adapted from SiCaptureSpectrum)\n",
    "\n",
    "In order to assess the expected signal in a capture experiment that could be run at UMN, Anthony simulated silicon capture cascades in the correct ratios according to the isotopic abundance and probabilities of specific cascades.\n",
    "\n",
    "He modeled the 47 most important cascades for silicon as accurately as possible.  Information about the capture probabilities has come from [this paper][SiCascade].  Knowledge of the level lifetimes has generally come from [Brookhaven National Lab's interactive chart of the nuclides][nndc].\n",
    "\n",
    "[SiCascade]: https://journals.aps.org/prc/abstract/10.1103/PhysRevC.46.972 \"Si Capture Reference\"\n",
    "[nndc]: https://www.nndc.bnl.gov/chart/ \"Interactive Chart of the Nuclides\"\n",
    "\n",
    "Some lifetimes require estimation because they are unknown.  In these cases Anthony used an [empirical relation][WeissCalc] based on the [Weisskopf estimates][WeissEst] for the lifetimes of nuclear states given the multipolarity of their decays.  He often use the shorthand \"fast\" or \"slow\" estimates, this means that for a given level he would either use the allowed multipolarity that gives the shortest transition lifetime (fast estimate) or the allowed multipolarity that would give the longest transition time (slow estimates). \n",
    "\n",
    "[WeissCalc]: https://www.sciencedirect.com/science/article/pii/S0550306X66800113 \"Definition of Weisskopf Units\"\n",
    "[WeissEst]: https://journals.aps.org/pr/abstract/10.1103/PhysRev.83.1073 \"Phys. Rev. 1951 Paper of Weisskopf Estimate\"\n",
    "\n",
    "Slow transitions generally result in more well-resolved peaks because the recoiling excited state has time to deposit all of its energy before recoiling off of another gamma.  In that case the sum of the recoil energies is constant in a given cascade.  If any of the decays happen \"in flight\" then the recoils may not deposit all of their energies and can smear the spectrum.  It is tacitly assumed that all gammas will leave the volume _without_ interacting.  This is a very good approximation for a small detector. \n",
    "\n",
    "Let's load up some simulated cascade data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "lifetimes='fast'\n",
    "Ncascades='200k' #'2M' or '200k'\n",
    "\n",
    "#load up some cascade simulated data; 20k cascade events\n",
    "with open('/data/chocula/villaa/cascadeSimData/normsi_{0}_{1}.pkl'.format(lifetimes, Ncascades),'rb') as readFile:\n",
    "      cdata=pkl.load(readFile,encoding='latin1')\n",
    "\n",
    "print(cdata.keys())\n",
    "print(cdata['totalevents'])\n",
    "\n",
    "#https://zzz.physics.umn.edu/cdms/doku.php?id=cdms:k100:run_summary:run_68:run_68_n125:full_signal_fit&#efficiencies_and_live_time\n",
    "tlive_ng = cdata['totalevents']/0.218 #[s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the keys in the associated dictionary is linked to a [NumPy][numpy] data structure with structure and meaning as listed in the table below. \n",
    "\n",
    "[numpy]: http://www.numpy.org/ \"NumPy\"\n",
    "\n",
    "key name|NumPy structure|Description \n",
    ":-|:-|:-\n",
    "totalevents|integer value|total number of simulated cascades\n",
    "n|integer array with shape (totalevents,)|number of steps in cascade\n",
    "cid|integer array with shape (totalevents,)|identifier for specific cascade based on simulation input\n",
    "Elev|double array with shape (totalevents,n)|level energy after each recoil in keV\n",
    "taus|double array with shape (totalevents,n)|lifetime of each level in fs\n",
    "E|double array with shape (totalevents,n)|energy of recoiling ion at the beginning of each \"step\" in eV\n",
    "delE|double array with shape (totalevents,n)|energy deposited at each step in eV\n",
    "I|integer array with shape (totalevents,n)|ionization according to Lindhard model for each step in e/h pairs\n",
    "Ei|double array with shape (totalevents,n)|effective ionization in electron-equivalent energy for each step in eV$_{ee}$\n",
    "time|double array with shape (totalevents,n)|absolute time relative to capture time of decay of this step in fs\n",
    "Eg |double array with shape (totalevents,n)|gamma energy modifying this step's recoil in MeV\n",
    "cEscape |boolean array with shape (totalevents,)|True if all the gammas escaped the detector without interacting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Expected Capture Spectrum\n",
    "\n",
    "Since the capture events consist mostly of composite nuclear recoils, we must decide how to account for the ionization yield correctly for each cascade step.  Assume a nuclear recoil created by a cascade step begins at energy $E_0$ and deposits energy $\\delta E$ in the step. We would then expect the ionization to be that which would have been given from a deposited energy of $E_0$ less that which would have been given from a deposit of $E_0 - \\delta E$.  This simply assumes that the ionization is independent of the slowing-down history of the particle, or, that any recoil of energy $E$ will give a reproducible amount of ionization regardless of the circumstances of its creation.\n",
    "\n",
    "This means that the amount of ionization energy in a given step, i, is\n",
    "\n",
    "$E_{ion,i} = E_{0,i}Y(E_{0,i}) - (E_{0,i}-\\delta E_i)Y(E_{0,i}-\\delta E_i)$\n",
    "\n",
    "and the total number created in an event is simply: $N_{tot} = \\sum N_i = \\sum \\frac{E_{ion,i}}{\\varepsilon}$\n",
    "\n",
    "Similarly, we will assume that the variance of each step is: $\\sigma_i^2 = F_{NR}N_i$ and that hits are uncorrelated so that the total variance of the event is $\\sigma_{tot}^2 = \\sum \\sigma_i^2 $ so that, for an energy-independent Fano factor, we have $\\sigma_{tot}^2=F_{NR}N_{tot}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yield and resolution\n",
    "For each event we need to apply some yield and Fano factor. We also apply a resolution function. \n",
    "\n",
    "### Calculations are done as follows:\n",
    "\n",
    "#### ER\n",
    "1. Sum deposited hit energies to get total recoil energy, E_recoil\n",
    "2. Apply Fano to total recoil energy\n",
    "3. Calculate Eee = E_recoil*(1+YV/eps)/(1+V/eps)\n",
    "4. Apply resolution function (not including Fano contribution) to E_ee.\n",
    "\n",
    "#### NR, (n,gamma)\n",
    "1. Get ER energy deposited from each hit\n",
    "2. Apply Fano to each hit to get N\n",
    "3. E_NTL + E_recoil to get Etot for each hit\n",
    "4. Sum Etot for each event\n",
    "5. Convert to E_ee by dividing out G_NTL = 1+V/eps\n",
    "6. Apply resolution function (not including Fano contribution) to E_ee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import damic_y as dy \n",
    "\n",
    "#a spline extrapolation to DAMIC data\n",
    "damic_y = dy.getDAMICy()\n",
    "damic_yv = np.vectorize(damic_y) #vectorize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choices\n",
    "from scipy.special import erf\n",
    "\n",
    "V = 125 #R68 bias voltage\n",
    "eps = 3.8 #eV for silicon\n",
    "Eg = 1.12 #eV bandgap\n",
    "G_NTL = (1+(V/eps)) #NTL gain\n",
    "F = 0.1161 #silicon value taken from https://www.sciencedirect.com/science/article/pii/S0168900297009650\n",
    "\n",
    "#Some yield model\n",
    "def Yield(Er, par):\n",
    "    #return par[0]+(par[1]-par[0])/(1+par[2]/Er)\n",
    "    #return damic_y(Er)\n",
    "    return yL(Er, par[0])\n",
    "    #return ySorCon(Er, par[0], par[1])\n",
    "    #return yChav(Er, par[0], par[1])\n",
    "    \n",
    "#Lindhard yield\n",
    "#http://gymarkiv.sdu.dk/MFM/kdvs/mfm%2030-39/mfm-33-10.pdf\n",
    "#k = 0.133Z^(2/3)A^(âˆ’1/2)\n",
    "#This is only decent for eps>0.01 => Er>400 eV\n",
    "def yL(Er, k):\n",
    "    Z=14.\n",
    "    eps = 11.5*Er/1000*Z**(-7./3)\n",
    "    g = 3.*eps**0.15 + 0.7*eps**0.6 + eps\n",
    "    return (k*g)/(1+k*g)\n",
    "\n",
    "#Lindhard w/ Chavarria tweak\n",
    "#https://arxiv.org/pdf/1803.02903.pdf\n",
    "def yChav(Er,k,a):\n",
    "    return 1/(1/(a*Er)+1/yL(Er,k))\n",
    "\n",
    "#Sorenson: Lindhard + constant\n",
    "#https://journals.aps.org/prd/pdf/10.1103/PhysRevD.91.083509\n",
    "def ySorCon(Er,k,q):\n",
    "    Z=14.\n",
    "    eps = 11.5*Er/1000*Z**(-7./3)\n",
    "    #This can go negative for large q/eps\n",
    "    return np.maximum(0,yL(Er,k)-q/eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "E_test=np.logspace(-1,3,1000)\n",
    "plt.plot(E_test,damic_y(E_test),label='Damic')\n",
    "plt.plot(E_test,yL(E_test,0.179),label='Lindhard')\n",
    "plt.plot(E_test,ySorCon(E_test,0.179,1e-6),label='Sorensen')\n",
    "plt.plot(E_test,yChav(E_test,0.179,0.2),label='Chavarria')\n",
    "\n",
    "plt.gca().set_xlabel(r'$E_{recoil}$ [eV]')\n",
    "plt.gca().set_ylabel('Y')\n",
    "plt.gca().set_xscale('log')\n",
    "plt.gca().set_yscale('log')\n",
    "plt.gca().set_ylim(1e-2,0.5)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "#plt.savefig('figures/R68_Y_fit/Ytest1.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "#N e/h pairs function\n",
    "#simplistic gaussian model for charge to get width and mean correct\n",
    "#discretize a normal distribution, don't allow negative N\n",
    "def getNeh(E,eps,F):\n",
    "    #TMP\n",
    "    #return np.random.normal((E/eps),np.sqrt(F*(E/eps)),np.shape(E))\n",
    "    N=np.round(np.random.normal((E/eps),np.sqrt(F*(E/eps)),np.shape(E)))\n",
    "    return np.maximum(0,N)\n",
    "\n",
    "#Resolution functions\n",
    "\n",
    "#Detector Resolution function\n",
    "def sigma_ee(E,sigma0,B,A):\n",
    "    return np.sqrt(sigma0**2 + B*E + (A*E)**2)\n",
    "\n",
    "#Draw a value from the resolution distribution\n",
    "#Option to include effect of OF bias at low energies\n",
    "def getSmeared(E, doLowEbias=False):\n",
    "    #Params from Matt's Bkg resolution fit:\n",
    "    #https://zzz.physics.umn.edu/cdms/doku.php?id=cdms:k100:run_summary:run_68:run_68_panda:calibration#resolution_versus_energy\n",
    "    sigma0=10.27 #eV \n",
    "    B=0.627*3.8 #This includes FANO\n",
    "    B_1=B-F*eps\n",
    "    A=0 #TODO: This part has not been fit yet!\n",
    "\n",
    "    #Ignore low energy bias for faster execution\n",
    "    if not doLowEbias:\n",
    "        return np.random.normal(E,sigma_ee(E,sigma0,B_1,A),np.shape(E))\n",
    "    else:\n",
    "        #Low energy OF bias\n",
    "        #From Nick's fit to PuBe noise wall\n",
    "        N=3.83\n",
    "\n",
    "        Esmeared=np.array([])\n",
    "        for Ei in E:\n",
    "            s_ee=sigma_ee(Ei,sigma0,B,A)\n",
    "            vals=np.linspace(Ei-5*s_ee,Ei+5*s_ee,100)\n",
    "            weights=P_OF_max(vals,Ei,s_ee,N)\n",
    "            Esmeared=np.append(Esmeared,choices(vals, weights)[0])\n",
    "        return Esmeared\n",
    "\n",
    "#OF resolution function\n",
    "#Returns probability of Ahat given A and N indpendent bins when selecting the max Ahat\n",
    "#See http://www.hep.umn.edu/cdms/cdms_restricted/K100/analysis/OF_bias_theory.html\n",
    "def P_OF_max(Ahat,A,sigma,N):\n",
    "    term1=P_OF0(Ahat,A,sigma)*(0.5+0.5*erf(Ahat/np.sqrt(2)/sigma))**(N-1)\n",
    "    term2A=(N-1)*P_OF0(Ahat,0,sigma)*(0.5+0.5*erf(Ahat/np.sqrt(2)/sigma))**(N-2)\n",
    "    term2B=0.5+0.5*erf((Ahat-A)/np.sqrt(2)/sigma)\n",
    "    return term1+term2A*term2B\n",
    "\n",
    "#PDF for OF0\n",
    "def P_OF0(Ahat,A,sigma):\n",
    "    return 1/np.sqrt(2*np.pi*sigma**2)*np.exp(-(Ahat-A)**2/2/sigma**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficiencies\n",
    "Trigger efficiency is the result of triggering on simulated event distributions.\n",
    "\n",
    "Cut efficiencies are estimated from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trigger efficiency\n",
    "#Note, this is a function of true pulse energy, before OF resolution effects\n",
    "fefftrig = open('data/r68_trigger_eff_1keV.txt')\n",
    "data_efftrig = np.asarray([x.split() for x in fefftrig.readlines()[1:]],dtype=np.float)\n",
    "fefftrig.close()\n",
    "\n",
    "def trigEff(E):\n",
    "    #https://zzz.physics.umn.edu/cdms/doku.php?id=cdms:k100:run_summary:run_68:run_68_trigger\n",
    "    #v3\n",
    "    #return erf(E/(np.sqrt(2.)*30.86))\n",
    "    #v4\n",
    "    #return 1-np.exp(-((E/57.3015)**1.07219))\n",
    "\n",
    "    #Final\n",
    "    return np.interp(E,data_efftrig[:,0],data_efftrig[:,1])\n",
    "    \n",
    "#Return uncertainties on the trigger efficiency.\n",
    "#These come from the stdev across multiple trigger sims which dominated statistical uncertainties in a given sim\n",
    "def dtrigEff(E):\n",
    "    return np.interp(E,data_efftrig[:,0],data_efftrig[:,2])\n",
    "\n",
    "\n",
    "#Cut Efficiencies\n",
    "eff_write = 0.617 #https://zzz.physics.umn.edu/cdms/doku.php?id=cdms:k100:run_summary:run_68:run_68_rateandlivetime#iii_read_efficiency\n",
    "deff_write = 0.004\n",
    "eff_tail = 0.8197 #http://www.hep.umn.edu/cdms/cdms_restricted/K100/analysis/Run68_CutEff_1/\n",
    "deff_tail = 0.0013\n",
    "eff_pileup = 0.9651 #http://www.hep.umn.edu/cdms/cdms_restricted/K100/analysis/Run68_CutEff_2/\n",
    "deff_pileup = 0.0013\n",
    "eff_trigburst = 0.9887 #http://www.hep.umn.edu/cdms/cdms_restricted/K100/analysis/Run68_RateCut_pt2/\n",
    "deff_trigburst = 0.0013 #Estimated from the out-of band passage which has uncertainty ~sqrt(2/N) and N~1.2e6\n",
    "\n",
    "#Spikey Cut Efficiency\n",
    "feffspike = open('data/r68_cspike_eff_1keV.txt')\n",
    "data_effspike = np.asarray([x.split() for x in feffspike.readlines()[1:]],dtype=np.float)\n",
    "feffspike.close()\n",
    "\n",
    "def spikeEff(E):\n",
    "    #http://www.hep.umn.edu/cdms/cdms_restricted/K100/analysis/Run68_SpikeEff/\n",
    "    return np.interp(E,data_effspike[:,0],data_effspike[:,1])\n",
    "\n",
    "#Return upper and lower spike cut uncertainties\n",
    "def dspikeEff(E):\n",
    "    dup = np.interp(E,data_effspike[:,0],data_effspike[:,2])\n",
    "    dlow = np.interp(E,data_effspike[:,0],data_effspike[:,3])\n",
    "    return np.stack((dup,dlow))\n",
    "\n",
    "#Chisq Cut Efficiency\n",
    "feffchi = open('data/r68_cchit_eff_1keV.txt')\n",
    "data_effchi = np.asarray([x.split() for x in feffchi.readlines()[1:]],dtype=np.float)\n",
    "feffchi.close()\n",
    "\n",
    "def chisqEff(E):\n",
    "    #v1\n",
    "    #eff_chisq = 0.645 #http://www.hep.umn.edu/cdms/cdms_restricted/K100/analysis/Run68_CutEff_2/\n",
    "    \n",
    "    #v2 w/Energy dependence\n",
    "    #http://www.hep.umn.edu/cdms/cdms_restricted/K100/analysis/Run68_CutEff_2/\n",
    "    return np.interp(E,data_effchi[:,0],data_effchi[:,1])\n",
    "\n",
    "#Return upper and lower chisq cut uncertainties\n",
    "def dchisqEff(E):\n",
    "    dup = np.interp(E,data_effchi[:,0],data_effchi[:,2])\n",
    "    dlow = np.interp(E,data_effchi[:,0],data_effchi[:,3])\n",
    "    return np.stack((dup,dlow))\n",
    "\n",
    "#Return the total cut efficiency curve\n",
    "def cutEff(E):\n",
    "    return eff_write*eff_tail*eff_pileup*eff_trigburst*spikeEff(E)*chisqEff(E)\n",
    "\n",
    "#Return the upper and lower total cut uncertainties\n",
    "#Adding asymm errors in quadrature is apparently naughty (https://www.slac.stanford.edu/econf/C030908/papers/WEMT002.pdf)\n",
    "#But they're not that asymmetric, so it's probably fine\n",
    "def dcutEff(E):\n",
    "    eff_write*eff_tail*eff_pileup*eff_trigburst*spikeEff(E)*chisqEff(E)\n",
    "    \n",
    "    dupsq = (deff_write/eff_write)**2 + (deff_tail/eff_tail)**2 + (deff_pileup/eff_pileup)**2 + \\\n",
    "    (deff_trigburst/eff_trigburst)**2 + (dspikeEff(E)[0]/spikeEff(E))**2 (dchisqEff(E)[0]/chisqEff(E))**2\n",
    "    dup = np.sqrt(dupsq)\n",
    "    \n",
    "    dlowsq = (deff_write/eff_write)**2 + (deff_tail/eff_tail)**2 + (deff_pileup/eff_pileup)**2 + \\\n",
    "    (deff_trigburst/eff_trigburst)**2 + (dspikeEff(E)[1]/spikeEff(E))**2 (dchisqEff(E)[1]/chisqEff(E))**2\n",
    "    dlow = np.sqrt(dlowsq)\n",
    "    \n",
    "    return np.stack((dup,dlow))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_test=np.linspace(0,5e2,1000)\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.plot(E_test,trigEff(E_test),color='C0')\n",
    "ax.fill_between(E_test,trigEff(E_test)-dtrigEff(E_test),trigEff(E_test)+dtrigEff(E_test),alpha=0.5,color='C0')\n",
    "\n",
    "ax.set_xlabel(r'$E_{recoil}$ [eV]')\n",
    "ax.set_ylabel('Trigger Efficiency')\n",
    "\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.axhline(eff_write, color = next(ax._get_lines.prop_cycler)['color'], label='write Efficiency')\n",
    "ax.axhspan(eff_write-deff_write, eff_write+deff_write, alpha=0.5)\n",
    "\n",
    "ax.axhline(eff_tail, color = next(ax._get_lines.prop_cycler)['color'], label='ctail Efficiency')\n",
    "ax.axhspan(eff_tail-deff_tail, eff_tail+deff_tail, alpha=0.5)\n",
    "\n",
    "ax.axhline(eff_pileup, color = next(ax._get_lines.prop_cycler)['color'], label='cpileup Efficiency')\n",
    "ax.axhspan(eff_pileup-deff_pileup, eff_pileup+deff_pileup, alpha=0.5)\n",
    "\n",
    "ax.axhline(eff_trigburst, color = next(ax._get_lines.prop_cycler)['color'], label='ctrigburst Efficiency')\n",
    "ax.axhspan(eff_trigburst-deff_trigburst, eff_trigburst+deff_trigburst, alpha=0.5)\n",
    "\n",
    "line,=ax.plot(E_test, spikeEff(E_test), label='cSpikey Efficiency')\n",
    "ax.fill_between(E_test, spikeEff(E_test)-dspikeEff(E_test)[1], spikeEff(E_test)+dspikeEff(E_test)[0], \n",
    "                alpha=0.5, color=line.get_color())\n",
    "\n",
    "line,=ax.plot(E_test,chisqEff(E_test), label='cchit Efficiency')\n",
    "ax.fill_between(E_test, chisqEff(E_test)-dchisqEff(E_test)[1], chisqEff(E_test)+dchisqEff(E_test)[0], \n",
    "                alpha=0.5, color=line.get_color())\n",
    "\n",
    "line,=ax.plot(E_test, cutEff(E_test), label='Total Cut Efficiency')\n",
    "ax.fill_between(E_test, cutEff(E_test)-cutEff(E_test)[1], cutEff(E_test)+cutEff(E_test)[0], \n",
    "                alpha=0.5, color=line.get_color())\n",
    "ax.set_xlabel(r'$E_{recoil}$ [eV]')\n",
    "ax.set_ylabel('Efficiency')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "#plt.savefig('figures/R68_Y_fit/cut_effs.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select energies according to trigger efficiency\n",
    "#Return whether energies passed the trigger\n",
    "def passTrig(E):\n",
    "    return np.random.random(E.shape)<trigEff(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply yield, resolution, efficiencies etc. to simulated data to get an instance of a simulated E_ee spectrum\n",
    "\n",
    "#Ypar: Yield function parameters passed to Yield() function defined above\n",
    "#F_NR: Nuclear recoil Fano factor\n",
    "#fng_scale: (n,gamma) spectrum scaling factor\n",
    "#fnrer_scale: Scaling of NR+ER sim spectrum\n",
    "#doDetRes: whether to apply detector resolution function\n",
    "\n",
    "def buildSpectra(Ypar, F_NR, fng_scale=1, fnrer_scale=1, doDetRes=True):\n",
    "    #fudge = (n,gamma) rate normalization\n",
    "    global evec_nr_nocap\n",
    "    global evec_er_nocap\n",
    "    global cdata\n",
    "    \n",
    "    global E_PuBe\n",
    "    global E_Bkg\n",
    "    \n",
    "    ###############\n",
    "    #Yield, Fano, Resolution\n",
    "    ###############\n",
    "    #NR, No capture\n",
    "    Evec_nr_nocap_er = evec_nr_nocap*Yield(evec_nr_nocap,Ypar) #ER energy from NRs\n",
    "    \n",
    "    #Number of e/h pairs produced in each event\n",
    "    Nvec_nr_nocap = getNeh(Evec_nr_nocap_er,eps,F_NR)\n",
    "    #get the total phonon energies\n",
    "    Etvec_nr_nocap = evec_nr_nocap + Nvec_nr_nocap*V\n",
    "    #Sum event and convert to eVee scale by making the gamma assumption\n",
    "    Eee_nr_nocap  = np.sum(Etvec_nr_nocap,1)/G_NTL\n",
    "    \n",
    "    #Apply trigger logic to randomly drop events according to their energies\n",
    "    Eee_nr_nocap = Eee_nr_nocap[passTrig(Eee_nr_nocap)]\n",
    "    \n",
    "    if doDetRes:\n",
    "        Eee_nr_nocap = getSmeared(Eee_nr_nocap)#add resolution\n",
    "    #print('Eee_nr_nocap[:10]',Eee_nr_nocap[:10])\n",
    "    #print('Eee_nr_nocap.shape:',Eee_nr_nocap.shape)\n",
    "    #print('Eee_nr_nocap[Eee_nr_nocap*Y<2000].shape:',Eee_nr_nocap[Eee_nr_nocap*Y<2000].shape)\n",
    "\n",
    "    #ER, No capture\n",
    "    #Number of e/h pairs produced in event\n",
    "    E_er_nocap_er = np.sum(evec_er_nocap,1) #ER energy from ERs\n",
    "    N_er_nocap = getNeh(E_er_nocap_er,eps,F)\n",
    "    #get the total phonon energy\n",
    "    Et_er_nocap = N_er_nocap*V + E_er_nocap_er\n",
    "    #convert to eVee scale\n",
    "    Eee_er_nocap  = Et_er_nocap/G_NTL\n",
    "    \n",
    "    #Apply trigger logic to randomly drop events according to their energies\n",
    "    Eee_er_nocap = Eee_er_nocap[passTrig(Eee_er_nocap)]\n",
    "    \n",
    "    if doDetRes:\n",
    "        Eee_er_nocap = getSmeared(Eee_er_nocap)#add resolution\n",
    "    #print('Eee_er_nocap[:10]',Eee_er_nocap[:10])\n",
    "    #print('Eee_er_nocap.shape:',Eee_er_nocap.shape)\n",
    "    #print('Eee_er_nocap[Eee_er_nocap<2000].shape:',Eee_er_nocap[Eee_er_nocap<2000].shape)\n",
    "    \n",
    "    #Convert Simulated n,gamma hits to eVee events\n",
    "    #Select only events where the gamma escapes. Assume the rest end up at high eVee\n",
    "    #Prepare for the computation\n",
    "    E_ng = cdata['E'][cdata['cEscape']]\n",
    "    dE_ng = cdata['delE'][cdata['cEscape']]\n",
    "    E_ng_er = E_ng*Yield(E_ng,Ypar) - (E_ng-dE_ng)*Yield(E_ng-dE_ng,Ypar)\n",
    "    #print('E_ng_er.shape:',E_ng_er.shape)\n",
    "        \n",
    "    #simplistic gaussian model for charge to get width and mean correct\n",
    "    N_ng = getNeh(E_ng_er,eps,F_NR)\n",
    "\n",
    "    #get the total phonon energy\n",
    "    Et_ng_step = N_ng*V + dE_ng\n",
    "    Et_ng = np.sum(Et_ng_step,1)\n",
    "\n",
    "    #convert to eVee scale by making the gamma assumption\n",
    "    Eee_ng  = Et_ng/G_NTL\n",
    "    \n",
    "    #Apply trigger logic to randomly drop events according to their energies\n",
    "    Eee_ng = Eee_ng[passTrig(Eee_ng)]\n",
    "    \n",
    "    if doDetRes:\n",
    "        Eee_ng = getSmeared(Eee_ng) #add resolution\n",
    "    #print('Eee_ng[:10]',Eee_ng[:10])\n",
    "    #print('Eee_ng.shape:',Eee_ng.shape)\n",
    "    #print('Eee_ng[Eee_ng*Y<2000].shape:',Eee_ng[Eee_ng*Y<2000].shape)\n",
    "    ###############\n",
    "    #Binning\n",
    "    ###############\n",
    "    Emax = 2000 #eVee\n",
    "    Ebins=np.linspace(0,Emax,201)\n",
    "    Ebin_ctr=(Ebins[:-1]+Ebins[1:])/2\n",
    "\n",
    "    #Measured\n",
    "    n_E_PuBe,_ = np.histogram(E_PuBe*1e3,bins=Ebins)\n",
    "    n_E_Bkg,_ = np.histogram(E_Bkg*1e3,bins=Ebins)\n",
    "\n",
    "    #Simulated\n",
    "    n_Eee_nr_nocap,_ = np.histogram(Eee_nr_nocap,bins=Ebins)\n",
    "    n_Eee_er_nocap,_ = np.histogram(Eee_er_nocap,bins=Ebins)\n",
    "    n_Eee_ng,_ = np.histogram(Eee_ng,bins=Ebins)\n",
    "\n",
    "          \n",
    "    #print('sum(n_Eee_nr_nocap):',np.sum(n_Eee_nr_nocap))\n",
    "    #print('sum(n_Eee_er_nocap):',np.sum(n_Eee_er_nocap))\n",
    "    #print('sum(n_Eee_ng):',np.sum(n_Eee_ng))\n",
    "    \n",
    "    ###############\n",
    "    #Livetime, efficiencies, etc.\n",
    "    ###############\n",
    "    \n",
    "    #Livetime\n",
    "    N_er = n_Eee_er_nocap/tlive_g4\n",
    "    N_nr = n_Eee_nr_nocap/tlive_g4\n",
    "    N_ng = n_Eee_ng/tlive_ng\n",
    "    N_PuBe = n_E_PuBe/tlive_PuBe\n",
    "    N_bkg = n_E_Bkg/tlive_bkg\n",
    "    \n",
    "    eff_cuts = cutEff(Ebin_ctr)\n",
    "    \n",
    "    #Apply these to the simulated data\n",
    "    N_er = N_er*eff_cuts*fnrer_scale\n",
    "    N_nr = N_nr*eff_cuts*fnrer_scale\n",
    "    N_ng = N_ng*eff_cuts*fng_scale\n",
    "    \n",
    "    return (Ebins, N_er, N_nr, N_ng, N_PuBe, N_bkg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Averaged spectra\n",
    "Here we calculate the average simulated spectra instead of a single instance of the simulated spectra. This can be achieved by either averaging a large number of simulated spectra with the same Y, F_NR, etc. or by smearing each simulated event with the resolution functions. The mean, electron-equivalent energy, for a recoil of energy Er is:\n",
    "\n",
    "$<E_{ee}> = (E_r + Y*E_r/\\epsilon*V)/G_{NTL} = E_r\\frac{1 + Y*V/\\epsilon}{1+V/\\epsilon}$\n",
    "\n",
    "and the width is (http://www.hep.umn.edu/cdms/cdms_restricted/K100/analysis/Peak_Widths.pdf):\n",
    "\n",
    "$\\sigma_{ee}^2 = E_{ion} \\frac{FV^2}{\\epsilon}/(1 + V/\\epsilon)^2$\n",
    "\n",
    "The ER hits in a given 'EV' are all considered part of the same event, which deposited some total recoil energy, Er, which is the sum of those hits. The Yield and Fano used is that for the total Er for that event. \n",
    "\n",
    "The NR hits in a given 'EV' are also part of the same event, but each is treated as a separate recoil, with Yield and Fano applied to each hit before the total event energy is summed.\n",
    "\n",
    "Note, there are still some hits in some 'EV' which are well separated in time. These should eventually be split into separate events.\n",
    "\n",
    "The (n,gamma) events are a little trickier since they may involve several overlapping NRs. But, as explained above, we'll calculate the ionization energy in interaction i as\n",
    "\n",
    "$E_{ion,i} = E_{0,i}Y(E_{0,i}) - (E_{0,i}-\\delta E_i)Y(E_{0,i}-\\delta E_i)$,\n",
    "\n",
    "the total number created in an event as: $N_{tot} = \\sum N_i$\n",
    "and $\\sigma_{tot}^2=F_{NR}N_{tot}$. This assumes the individual hits are uncorrelated and F_NR is constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note one expected difference is the way hits with $E_{ion,i}/\\epsilon = \\mathcal{O}(1)$ are handled. When simulating a single instance of such a hit (i.e. in `buildSpectra`), we use the function `getNeh` to draw the number of e/h pairs from a normal distribution, with values <0 disallowed. This function also rounds to the nearest Neh. However, in the `buildAvgSpectra` function, we don't round or implement that hard cutoff, i.e. the distribution of possible amplitudes may have a substantial tail below 0 energy for depositions near 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Integral of gaussian\n",
    "def gausInt(mu, sigma, xlow, xhi):\n",
    "    return np.where(sigma>0, 0.5*erf((mu-xlow)/np.sqrt(2*sigma**2)) - 0.5*erf((mu-xhi)/np.sqrt(2*sigma**2)), 1.0*((mu>xlow) & (mu<xhi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply yield, resolution, efficiencies etc. to simulated data\n",
    "# This version convolves fano and resolution functions with the spectrum, as opposed to drawing invividual values from \n",
    "# probability distributions for each hit.\n",
    "# As such, this function returns the average spectra, which will not, in general, have integer counts\n",
    "\n",
    "#Ypar: Yield function parameters passed to Yield() function defined above\n",
    "#F_NR: Nuclear recoil Fano factor\n",
    "#fng_scale: (n,gamma) spectrum scaling factor\n",
    "#fnrer_scale: Scaling of NR+ER sim spectrum\n",
    "#doDetRes: whether to apply detector resolution function\n",
    "\n",
    "#TODO: -Handle low energy Neh consistent with model used in buildSpectra\n",
    "def buildAvgSpectra(Ypar, F_NR, fng_scale=1, fnrer_scale=1, doDetRes=True):\n",
    "\n",
    "    global evec_nr_nocap\n",
    "    global evec_er_nocap\n",
    "    global cdata\n",
    "\n",
    "    global E_PuBe\n",
    "    global E_Bkg\n",
    "\n",
    "    ###############\n",
    "    #Binning\n",
    "    ###############\n",
    "    #E_ee binning\n",
    "    Eee_max = 2000 #eVee\n",
    "    Nbins_Eee = 200\n",
    "    Eee_bins = np.linspace(0,Eee_max,Nbins_Eee+1)#bin edges\n",
    "    Eee_binCtr = (Eee_bins[:-1]+Eee_bins[1:])/2\n",
    "    Eee_count = np.zeros(Nbins_Eee)\n",
    "\n",
    "    ###############\n",
    "    #Yield, Fano\n",
    "    ###############\n",
    "\n",
    "    ################\n",
    "    #NR, No capture\n",
    "\n",
    "    #<E_ee> in each hit\n",
    "    EeeMeanVec_nr_nocap = evec_nr_nocap*(1 + Yield(evec_nr_nocap,Ypar)*V/eps) / (1+V/eps)\n",
    "    #Sigma_ee^2 for each hit\n",
    "    #TMP\n",
    "    #SigmaEeeSqVec_nr_nocap = EeeMeanVec_nr_nocap*F_NR*eps/Yield(evec_nr_nocap,Ypar)*((1 + Yield(evec_nr_nocap,Ypar)*V/eps)/(1+V/eps))\n",
    "    SigmaEeeSqVec_nr_nocap = evec_nr_nocap*Yield(evec_nr_nocap,Ypar)*(V**2)*F_NR/eps/((1 + V/eps)**2)\n",
    "    \n",
    "    #energy==0 (no hit) may still generate Yield!=0, take care of that here\n",
    "    SigmaEeeSqVec_nr_nocap = np.where(evec_nr_nocap>0, SigmaEeeSqVec_nr_nocap, 0)\n",
    "    \n",
    "    #Sum hits assuming they're uncorrelated\n",
    "    EeeMean_nr_nocap=np.sum(EeeMeanVec_nr_nocap,1)\n",
    "    SigmaEee_nr_nocap = np.sqrt(np.sum(SigmaEeeSqVec_nr_nocap,1))\n",
    "    #print('EeeMean_nr_nocap[:10]',EeeMean_nr_nocap[:10])   \n",
    "    \n",
    "    #Reshape so we can broadcast against bin centers\n",
    "    EeeMean_nr_nocap = EeeMean_nr_nocap[:,np.newaxis]\n",
    "    SigmaEee_nr_nocap = SigmaEee_nr_nocap[:,np.newaxis]\n",
    "    #print('EeeMean_nr_nocap.shape:',EeeMean_nr_nocap.shape)\n",
    "    #print('EeeMean_nr_nocap[EeeMean_nr_nocap*Y<2000].shape:',EeeMean_nr_nocap[EeeMean_nr_nocap*Y<2000].shape)\n",
    "    \n",
    "    #Weighted spectra of Eee energies measured\n",
    "    Eee_counts_nr_nocap = gausInt(EeeMean_nr_nocap, SigmaEee_nr_nocap, Eee_bins[:-1], Eee_bins[1:])\n",
    "    #Total spectrum\n",
    "    Eee_hist_nr_nocap = np.sum(Eee_counts_nr_nocap,0)\n",
    "    #print('sum(Eee_hist_nr_nocap):',np.sum(Eee_hist_nr_nocap))\n",
    "    \n",
    "    ################\n",
    "    #ER, No capture\n",
    "\n",
    "    #Recoil energy, Er, in each event\n",
    "    ErMean_er_nocap = np.sum(evec_er_nocap,1)\n",
    "    EeeMean_er_nocap = ErMean_er_nocap\n",
    "    #print('EeeMean_er_nocap[:10]',EeeMean_er_nocap[:10])\n",
    "    #TMP\n",
    "    #SigmaEee_er_nocap = np.sqrt(EeeMean_er_nocap*F*eps)\n",
    "    SigmaEee_er_nocap = np.sqrt(ErMean_er_nocap*(V**2)*F/eps/((1 + V/eps)**2))\n",
    "\n",
    "\n",
    "    #Reshape so we can broadcast against bin centers\n",
    "    EeeMean_er_nocap = EeeMean_er_nocap[:,np.newaxis]\n",
    "    SigmaEee_er_nocap = SigmaEee_er_nocap[:,np.newaxis]\n",
    "    #print('EeeMean_er_nocap.shape:',EeeMean_er_nocap.shape)\n",
    "    #print('EeeMean_er_nocap[EeeMean_er_nocap<2000].shape:',EeeMean_er_nocap[EeeMean_er_nocap<2000].shape)\n",
    "    \n",
    "    #Weighted spectra of Eee energies measured\n",
    "    Eee_counts_er_nocap = gausInt(EeeMean_er_nocap, SigmaEee_er_nocap, Eee_bins[:-1], Eee_bins[1:])\n",
    "    #Total spectrum\n",
    "    Eee_hist_er_nocap = np.sum(Eee_counts_er_nocap,0)\n",
    "    #print('sum(Eee_hist_er_nocap):',np.sum(Eee_hist_er_nocap))\n",
    "    \n",
    "    ################\n",
    "    #N,gamma\n",
    "    #Convert Simulated n,gamma hits to eVee events\n",
    "    #Select only events where the gamma escapes. Assume the rest end up at high eVee\n",
    "    #Prepare for the computation\n",
    "    evec_ng = cdata['E'][cdata['cEscape']]\n",
    "    devec_ng = cdata['delE'][cdata['cEscape']]\n",
    "    #Still separate hits here.\n",
    "\n",
    "    EIonMeanVec_ng = evec_ng*Yield(evec_ng,Ypar) - (evec_ng-devec_ng)*Yield(evec_ng-devec_ng,Ypar)#Ionization energy\n",
    "    ErMeanVec_ng = devec_ng#Recoil energy\n",
    "    EeeMeanVec_ng = (ErMeanVec_ng + EIonMeanVec_ng*V/eps) / (1+V/eps)#Electon-equivalent\n",
    "    #print('EeeMean_ng[:10]',EeeMean_ng[:10])\n",
    "    #TODO: This is not quite correct if dE/E<<1, but may be close enough since that's rare?\n",
    "    #TMP\n",
    "    #SigmaEeeSqVec_ng = EeeMeanVec_ng * (F_NR*eps/Yield(ErMeanVec_ng,Ypar)) * ((1 + Yield(ErMeanVec_ng,Ypar)*V/eps)/(1+V/eps))\n",
    "    SigmaEeeSqVec_ng = EIonMeanVec_ng*(V**2)*F_NR/eps/((1 + V/eps)**2)\n",
    "\n",
    "    #energy==0 (no hit) may still generate Yield!=0, take care of that here\n",
    "    SigmaEeeSqVec_ng = np.where(evec_ng>0, SigmaEeeSqVec_ng, 0)\n",
    "\n",
    "    #Sum hits assuming they're uncorrelated\n",
    "    EeeMean_ng = np.sum(EeeMeanVec_ng,1)\n",
    "    SigmaEee_ng = np.sqrt(np.sum(SigmaEeeSqVec_ng,1))  \n",
    "    \n",
    "    #Reshape so we can broadcast against bin centers\n",
    "    EeeMean_ng = EeeMean_ng[:,np.newaxis]\n",
    "    SigmaEee_ng = SigmaEee_ng[:,np.newaxis]\n",
    "    #print('EeeMean_ng.shape:',EeeMean_ng.shape)\n",
    "    #print('EeeMean_ng[EeeMean_ng*Y<2000].shape:',EeeMean_ng[EeeMean_ng*Y<2000].shape)\n",
    "\n",
    "    #Weighted spectra of Eee energies measured\n",
    "    Eee_counts_ng = gausInt(EeeMean_ng, SigmaEee_ng, Eee_bins[:-1], Eee_bins[1:])\n",
    "    #Total spectrum\n",
    "    Eee_hist_ng = np.sum(Eee_counts_ng,0)\n",
    "    #print('sum(Eee_hist_ng):',np.sum(Eee_hist_ng))\n",
    "    \n",
    "    ###############\n",
    "    #Simulated Trigger Efficiency\n",
    "    ###############\n",
    "    tEff=trigEff(Eee_binCtr)\n",
    "    Eee_hist_er_nocap = Eee_hist_er_nocap*tEff\n",
    "    Eee_hist_nr_nocap = Eee_hist_nr_nocap*tEff\n",
    "    Eee_hist_ng  = Eee_hist_ng*tEff\n",
    "    \n",
    "    if doDetRes:\n",
    "        ###############\n",
    "        #Detector Resolution\n",
    "        ###############\n",
    "        #Resolution model\n",
    "        #sigma_ee^2 = sigma0^2 + B*Eee + (A*Eee)^2 (from CDMSlite paper)\n",
    "        #B contains Fano contribution and maybe other stuff\n",
    "        # B = F*epsilon + B_1\n",
    "\n",
    "        #Params from Matt's Bkg resolution fit:\n",
    "        #https://zzz.physics.umn.edu/cdms/doku.php?id=cdms:k100:run_summary:run_68:run_68_panda:calibration#resolution_versus_energy\n",
    "        sigma0=10.27 #eV \n",
    "        B=0.627*3.8 #This includes ER FANO, which we've already applied above\n",
    "        A=0 #TODO: This part has not been fit yet!\n",
    "        B_1=B-F*eps\n",
    "\n",
    "        #Apply this resolution function to the binned data\n",
    "        #NR\n",
    "        ####\n",
    "        Eee_hist_nr_nocap_smeared = Eee_hist_nr_nocap[:,np.newaxis]*gausInt(Eee_binCtr[:,np.newaxis], \n",
    "                                                              sigma_ee(Eee_binCtr[:,np.newaxis],sigma0,B_1,A),\n",
    "                                                              Eee_bins[:-1],\n",
    "                                                              Eee_bins[1:])\n",
    "\n",
    "        Eee_hist_nr_nocap_smeared = np.sum(Eee_hist_nr_nocap_smeared,0)\n",
    "\n",
    "        #ER\n",
    "        ####\n",
    "        Eee_hist_er_nocap_smeared = Eee_hist_er_nocap[:,np.newaxis]*gausInt(Eee_binCtr[:,np.newaxis], \n",
    "                                                              sigma_ee(Eee_binCtr[:,np.newaxis],sigma0,B_1,A),\n",
    "                                                              Eee_bins[:-1],\n",
    "                                                              Eee_bins[1:])\n",
    "\n",
    "        Eee_hist_er_nocap_smeared = np.sum(Eee_hist_er_nocap_smeared,0)\n",
    "\n",
    "        #(n,g)\n",
    "        ######\n",
    "        Eee_hist_ng_smeared = Eee_hist_ng[:,np.newaxis]*gausInt(Eee_binCtr[:,np.newaxis], \n",
    "                                                              sigma_ee(Eee_binCtr[:,np.newaxis],sigma0,B_1,A),\n",
    "                                                              Eee_bins[:-1],\n",
    "                                                              Eee_bins[1:])\n",
    "\n",
    "        Eee_hist_ng_smeared = np.sum(Eee_hist_ng_smeared,0)\n",
    "\n",
    "        Eee_hist_nr_nocap = Eee_hist_nr_nocap_smeared\n",
    "        Eee_hist_er_nocap = Eee_hist_er_nocap_smeared\n",
    "        Eee_hist_ng = Eee_hist_ng_smeared\n",
    "    \n",
    "    ###############\n",
    "    #Measured\n",
    "    Eee_hist_PuBe,_ = np.histogram(E_PuBe*1e3,bins=Eee_bins)\n",
    "    Eee_hist_Bkg,_ = np.histogram(E_Bkg*1e3,bins=Eee_bins)\n",
    "    \n",
    "    ###############\n",
    "    #Livetime, efficiencies, etc.\n",
    "    ###############\n",
    "\n",
    "    #Livetime\n",
    "    N_er = Eee_hist_er_nocap/tlive_g4 #[counts/bin/sec]\n",
    "    N_nr = Eee_hist_nr_nocap/tlive_g4\n",
    "    N_ng = Eee_hist_ng/tlive_ng\n",
    "    N_PuBe = Eee_hist_PuBe/tlive_PuBe\n",
    "    N_bkg = Eee_hist_Bkg/tlive_bkg\n",
    "        \n",
    "    eff_cuts = cutEff(Eee_binCtr)\n",
    "    \n",
    "    #Apply these to the simulated data\n",
    "    N_er = N_er*eff_cuts*fnrer_scale\n",
    "    N_nr = N_nr*eff_cuts*fnrer_scale\n",
    "    N_ng = N_ng*eff_cuts*fng_scale\n",
    "    \n",
    "    return (Eee_bins, N_er, N_nr, N_ng, N_PuBe, N_bkg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check resolution calculation\n",
    "sigma0=10.27 #eV \n",
    "B=0.627*3.8 #This includes ER FANO, which we've already applied above\n",
    "A=0 #TODO: This part has not been fit yet!\n",
    "B_1=B-F*eps\n",
    "Eb=np.linspace(-100,1800,300)\n",
    "plt.plot((Eb[:-1]+Eb[1:])/2,gausInt(0, sigma_ee(0,sigma0,B_1,A),Eb[:-1],Eb[1:]));\n",
    "plt.plot((Eb[:-1]+Eb[1:])/2,gausInt(500, sigma_ee(500,sigma0,B_1,A),Eb[:-1],Eb[1:]));\n",
    "plt.plot((Eb[:-1]+Eb[1:])/2,gausInt(1000, sigma_ee(1000,sigma0,B_1,A),Eb[:-1],Eb[1:]));\n",
    "plt.plot((Eb[:-1]+Eb[1:])/2,gausInt(1500, sigma_ee(1500,sigma0,B_1,A),Eb[:-1],Eb[1:]));\n",
    "plt.gca().set_xlabel('Energy [eVee]');\n",
    "plt.gca().set_ylabel('PDF');\n",
    "\n",
    "np.sum(gausInt(1000, sigma_ee(1000,sigma0,B_1,A),Eb[:-1],Eb[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check applying detector resolution to a spectrum\n",
    "\n",
    "Eb=np.arange(-0.5,1805,1) #If this goes too far negative, sigma breaks\n",
    "Eb_ctr=(Eb[:-1]+Eb[1:])/2\n",
    "Ec=np.zeros_like(Eb_ctr)\n",
    "Ec[(Eb_ctr==0) | (Eb_ctr==500) | (Eb_ctr==1000) | (Eb_ctr==1500)]=1\n",
    "\n",
    "Ec_smeared = Ec[:,np.newaxis]*gausInt(Eb_ctr[:,np.newaxis],sigma_ee(Eb_ctr[:,np.newaxis],sigma0,B_1,A),Eb[:-1],Eb[1:])\n",
    "Ec_smeared_hist = np.sum(Ec_smeared,0)\n",
    "\n",
    "#plt.step(Eb_ctr,Ec, where='mid',color='k', linestyle='-');\n",
    "plt.step(Eb_ctr,Ec_smeared_hist, where='mid',color='r', linestyle='-');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotSpectra(E_bins, N_er, N_nr, N_ng, N_PuBe, N_bkg, xrange=(0,1e3), yrange=(0,5e3), yscale='linear', thresh=None):\n",
    "    fig,axes = plt.subplots(1,1,figsize=(9.0,8.0),sharex=True)\n",
    "    ax1 = axes\n",
    "\n",
    "    dE = E_bins[1]-E_bins[0]\n",
    "    \n",
    "    Ec = (E_bins[:-1] + E_bins[1:]) / 2\n",
    "    \n",
    "    ax1.step(Ec,N_nr, where='mid',color='k', linestyle='-', \\\n",
    "             label='NR, no Capture', linewidth=2)\n",
    "\n",
    "    ax1.step(Ec,N_er, where='mid',color='r', linestyle='-', \\\n",
    "             label='ER, no Capture', linewidth=2)\n",
    "\n",
    "    ax1.step(Ec,N_ng, where='mid',color='b', linestyle='-', \\\n",
    "             label='(n,gamma)', linewidth=2)\n",
    "\n",
    "    ax1.step(Ec,N_nr+N_er+N_ng, where='mid',color='g', linestyle='-', \\\n",
    "             label='All Sims', linewidth=2)\n",
    "\n",
    "    err = np.sqrt( N_PuBe/tlive_PuBe + N_bkg/tlive_bkg )\n",
    "    ax1.errorbar(Ec,N_PuBe-N_bkg,yerr=[err,err], marker='o', markersize=6, \\\n",
    "                 ecolor='k',color='k', linestyle='none', label='Data-Bkg', linewidth=2)\n",
    "    \n",
    "    if thresh is not None:\n",
    "        ax1.axvline(thresh, color='m', linestyle='--', linewidth=2, label='Threshold')\n",
    "    \n",
    "    ax1.set_yscale(yscale)\n",
    "    ax1.set_xlim(*xrange)\n",
    "    ax1.set_ylim(*yrange)\n",
    "    ax1.set_xlabel('total deposited energy [eV$_{\\\\mathrm{ee}}$]',**axis_font)\n",
    "    #ax1.set_ylabel('counts',**axis_font)\n",
    "    ax1.set_ylabel('Events/bin/s',**axis_font)\n",
    "    ax1.grid(True)\n",
    "    ax1.yaxis.grid(True,which='minor',linestyle='--')\n",
    "    ax1.legend(loc=1,prop={'size':22})\n",
    "\n",
    "    for axis in ['top','bottom','left','right']:\n",
    "      ax1.spines[axis].set_linewidth(2)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Eee_bins, Eee_hist_er_nocap, Eee_hist_nr_nocap, Eee_hist_ng, Eee_hist_PuBe, Eee_hist_Bkg = buildAvgSpectra([0.001,0.2,20], 0.1, 2e-2, doDetRes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotSpectra(*buildSpectra([0.001,0.35,200], 0.5, 1e-2, doDetRes=True))\n",
    "#print('===============')\n",
    "#plotSpectra(*buildAvgSpectra([0.15,0.24,140], 2.4, fng_scale=1.5, fnrer_scale=1.5, doDetRes=True), yrange=(0,0.01))\n",
    "\n",
    "plotSpectra(*buildAvgSpectra([0.178, 3e-4], 4.0, fng_scale=1.6, fnrer_scale=1.6, doDetRes=True), yrange=(0,0.01))\n",
    "\n",
    "#plt.savefig(\"figures/R68_Y_fit/R68_Y_fit_Ytest2_1p5_tlive_detres.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Compare average of spectra instances to analytitcal average\n",
    "E_bins=list()\n",
    "N_er=list()\n",
    "N_nr=list()\n",
    "N_ng=list()\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    a,b,c,d,e,f = buildSpectra([0.178], 1, fng_scale=1.5, fnrer_scale=1.5, doDetRes=True);\n",
    "    E_bins.append(a)\n",
    "    N_er.append(b)\n",
    "    N_nr.append(c)\n",
    "    N_ng.append(d)\n",
    "    \n",
    "E_bins=np.array(E_bins)\n",
    "N_er=np.array(N_er)\n",
    "N_nr=np.array(N_nr)\n",
    "N_ng=np.array(N_ng)\n",
    "\n",
    "E_bins_avg, N_er_avg, N_nr_avg, N_ng_avg,_,_ = buildAvgSpectra([0.178], 1, fng_scale=1.5, fnrer_scale=1.5, doDetRes=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_ctr=(E_bins[0][1:]+E_bins[0][:-1])/2\n",
    "\n",
    "fig,ax = plt.subplots(3,3,figsize=(12.0,10.0),sharex=False)\n",
    "for iN_er in N_er:\n",
    "    ax[0,0].step(E_ctr,iN_er,where='mid');\n",
    "ax[0,1].step(E_ctr,np.mean(N_er,0),where='mid');\n",
    "ax[0,1].step(E_ctr,N_er_avg,where='mid');\n",
    "ax[0,2].step(E_ctr,N_er_avg/np.mean(N_er,0),where='mid');\n",
    "\n",
    "NRrange=slice(0,50)\n",
    "\n",
    "for iN_nr in N_nr:\n",
    "    ax[1,0].step(E_ctr[NRrange],iN_nr[NRrange],where='mid');\n",
    "#ax[1,0].set_ylim(0,5e3)\n",
    "#ax[1,0].set_yscale('log')\n",
    "ax[1,1].step(E_ctr[NRrange],np.mean(N_nr,0)[NRrange],where='mid');\n",
    "ax[1,1].step(E_ctr[NRrange],N_nr_avg[NRrange],where='mid');\n",
    "#ax[1,1].set_ylim(0,5e3)\n",
    "#ax[1,1].set_yscale('log')\n",
    "ax[1,2].step(E_ctr[NRrange],(N_nr_avg/np.mean(N_nr,0))[NRrange],where='mid');\n",
    "ax[1,2].set_ylim(0.9,1.1)\n",
    "\n",
    "for iN_ng in N_ng:\n",
    "    ax[2,0].step(E_ctr[NRrange],iN_ng[NRrange],where='mid');\n",
    "ax[2,1].step(E_ctr[NRrange],np.mean(N_ng,0)[NRrange],where='mid');\n",
    "ax[2,1].step(E_ctr[NRrange],N_ng_avg[NRrange],where='mid');\n",
    "#ax[2,1].set_yscale('log')\n",
    "ax[2,2].step(E_ctr[NRrange],(N_ng_avg/np.mean(N_ng,0))[NRrange],where='mid');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Scan\n",
    "All of the steps above got us to the point of comparing spectra. Let's define a goodness of fit quantity and then build a loop to fit for various parameters like those describing yield and NR Fano factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "#Chisq goodness of fit of spec_meas to spec_pred, given predicted uncertainties unc_pred\n",
    "def Chisq(spec_meas, spec_pred, unc_pred):\n",
    "    return ((spec_meas-spec_pred)**2)/(unc_pred**2)\n",
    "\n",
    "#Calculate the Poissonian likelihood of measuring spectrum spec_meas given the predicted spectrum spec_pred\n",
    "def PoisLikelihood(spec_meas, spec_pred):\n",
    "    PLi = [np.exp(-lamb)*(float(lamb)**n)/scipy.math.factorial(n) for n,lamb in zip(spec_meas,spec_pred)]\n",
    "    return np.prod(PLi)\n",
    "\n",
    "#Calculate the Log of Poissonian likelihood of measuring spectrum spec_meas given the predicted spectrum spec_pred\n",
    "def PoisLogLikelihood(spec_meas, spec_pred):\n",
    "    #PLLi = [ -n*lamb-np.log(float(scipy.math.factorial(n)))+n*np.log(float(lamb)) for n,lamb in zip(spec_meas,spec_pred)]\n",
    "    #Stirlings approx for large n\n",
    "    PLLi = [ -n*lamb-n*np.log(n)-n+n*np.log(lamb) for n,lamb in zip(spec_meas,spec_pred)]\n",
    "    return np.sum(PLLi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main fitting functions which will be passed to minimizers\n",
    "def FittingFunc(params):\n",
    "\n",
    "    #Lindhard\n",
    "    #Ypar0=params[0]\n",
    "    #scale=params[1]\n",
    "    #FNR=params[2]\n",
    "    \n",
    "    #E_bins, N_er, N_nr, N_ng, N_PuBe, N_bkg = buildAvgSpectra([Ypar0], FNR, fng_scale=scale, fnrer_scale=scale, doDetRes=True)\n",
    "    \n",
    "    #Chavarria or #Sorensen with constant\n",
    "    Ypar0=params[0]\n",
    "    Ypar1=params[1]\n",
    "    scale=params[2]\n",
    "    FNR=params[3]\n",
    "    E_bins, N_er, N_nr, N_ng, N_PuBe, N_bkg = buildAvgSpectra([Ypar0,Ypar1], FNR, fng_scale=scale, fnrer_scale=scale, doDetRes=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Only evaluate bins from the range [bin_low,bin_high)\n",
    "    bin_low=5\n",
    "    bin_high=101\n",
    "             \n",
    "    spec_meas = (N_PuBe-N_bkg)[bin_low:bin_high]\n",
    "    spec_pred = (N_er+N_nr+N_ng)[bin_low:bin_high]\n",
    "\n",
    "    unc_meas = np.sqrt( N_PuBe/tlive_PuBe + N_bkg/tlive_bkg )[bin_low:bin_high]\n",
    "    \n",
    "    #return PoisLogLikelihood(spec_meas,spec_pred)\n",
    "    return Chisq(spec_meas, spec_pred, unc_meas)\n",
    "\n",
    "#As above, but returns the sum of chisq\n",
    "def FittingFuncSum(params):\n",
    "    return np.sum(FittingFunc(params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scan over params\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "Nsample=20\n",
    "ks=np.linspace(0.1,0.25,Nsample) #Lindhard k\n",
    "scales=np.linspace(1,2,Nsample) #Simulation scale factor\n",
    "X,Y=np.meshgrid(ks,scales)\n",
    "Z=np.zeros_like(X)\n",
    "\n",
    "for i,ki in enumerate(ks):\n",
    "    print(np.round(10.0*i/Nsample),'%')\n",
    "    for j,scalei in enumerate(scales):\n",
    "        Z[j,i]=FittingFuncSum([ki,scalei])\n",
    "\n",
    "end = time.time()\n",
    "print((end - start)/60.,' min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import ticker\n",
    "import matplotlib.colors as colors\n",
    "fig,ax = plt.subplots(1,2,figsize=(15.0,6.0))\n",
    "\n",
    "cs0=ax[0].scatter(X, Y, c=Z, norm=colors.LogNorm(vmin=1e7, vmax=1e10))\n",
    "ax[0].set_xlabel('k')\n",
    "ax[0].set_ylabel('sim scale')\n",
    "cbar0=plt.colorbar(cs0, ax=ax[0])\n",
    "cbar0.set_label('Chisq', rotation=270)\n",
    "\n",
    "\n",
    "cs1=ax[1].contourf(X,Y,Z,norm=colors.LogNorm(vmin=Z.min(), vmax=Z.max()), levels=np.logspace(7,10,13))\n",
    "ax[1].set_xlabel('k')\n",
    "ax[1].set_ylabel('sim scale')\n",
    "cbar1=plt.colorbar(cs1, ax=ax[1])\n",
    "cbar1.set_label('Chisq', rotation=270)\n",
    "\n",
    "ax[1].axhline(1.60529231,color='r',linestyle='--')\n",
    "ax[1].axvline(0.18041768,color='r',linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"figures/R68_Y_fit/scan_2_chisq_wfit.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imin=np.unravel_index(np.argmin(Z, axis=None), Z.shape)\n",
    "print(X[imin], Y[imin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotSpectra(*buildAvgSpectra([X[imin]], 2.4, fng_scale=Y[imin], fnrer_scale=Y[imin], doDetRes=True), yrange=(0,0.01), thresh=50)\n",
    "#plt.savefig(\"figures/R68_Y_fit/scan_2_best_spectrum.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are expensive scans. Let's save the results\n",
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "saveScan=False\n",
    "\n",
    "if saveScan:\n",
    "    ifile = 0\n",
    "    while os.path.exists('data/R68_Y_fit/scan_{}.pkl'.format(ifile+1)):\n",
    "        ifile += 1\n",
    "        \n",
    "    print('data/R68_Y_fit/scan_{}.pkl'.format(ifile+1))\n",
    "    scanFile = open('data/R68_Y_fit/scan_{}.pkl'.format(ifile+1), 'wb')\n",
    "    pkl.dump(X, scanFile)\n",
    "    pkl.dump(Y, scanFile)\n",
    "    pkl.dump(Z, scanFile)\n",
    "    scanFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Functions\n",
    "Instead of scanning the parameter space, use a minimizer to find the best fit params\n",
    "\n",
    "First, try mimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "#Lindhard\n",
    "#x0=[0.15, 1.2, 2.4]  #k, sim_scale, F_NR\n",
    "#bounds=((0,1),(0,10),(0,5))\n",
    "\n",
    "#Chavarria\n",
    "#x0=[0.15, 0.2, 1.2, 2.4]  #k, a, sim_scale, F_NR\n",
    "#bounds=((0,1),(1./1000,1./0.1),(0,10),(0,5))\n",
    "\n",
    "#Sorensen with constant\n",
    "x0=[0.15, 1e-4, 1.2, 2.4]  #k, q, sim_scale, F_NR\n",
    "bounds=((0,1),(0,1e-3),(0,10),(0,5))\n",
    "\n",
    "start = time.time()\n",
    "res = minimize( FittingFuncSum, x0, bounds=bounds )\n",
    "end = time.time()\n",
    "print('{0:.1f} minutes'.format((end - start)/60.))\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimate parameter variances\n",
    "#Inverse of hessian is unscaled covariance matrix (https://e-maxx.ru/bookz/files/numerical_recipes.pdf)\n",
    "#Follow the perscription here to rescale it (https://stackoverflow.com/questions/43593592/errors-to-fit-parameters-of-scipy-optimize)\n",
    "\n",
    "ftol = 2.220446049250313e-09\n",
    "res.err=np.sqrt(max(1, abs(res.fun))*ftol*np.diag(res.hess_inv.todense()))\n",
    "\n",
    "#Lindhard\n",
    "par_names=[\"k\",\"sim_scale\",\"F_NR\"]\n",
    "#Chavarria\n",
    "#par_names=[\"k\",\"a\",\"sim_scale\",\"F_NR\"]\n",
    "for i,j,k in zip(par_names,res.x,res.err):\n",
    "    print(\"{0} = {1:.3f} +/- {2:.4e}\".format(i,j,k))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lindhard\n",
    "#plotSpectra(*buildAvgSpectra([res.x[0]], res.x[2], fng_scale=res.x[1], fnrer_scale=res.x[1], doDetRes=True), \n",
    "#            xrange=(0,1e3), yrange=(0,0.01), thresh=50)\n",
    "#plt.savefig(\"figures/R68_Y_fit/fit_Lind1_spectrum.png\")\n",
    "\n",
    "#Chavarria\n",
    "#plotSpectra(*buildAvgSpectra([res.x[0],res.x[1]], res.x[3], fng_scale=res.x[2], fnrer_scale=res.x[2], doDetRes=True), \n",
    "#            xrange=(0,1e3), yrange=(0,0.01), thresh=50)\n",
    "#plt.savefig(\"figures/R68_Y_fit/fit_Chav1_spectrum.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the best-fit yield models\n",
    "#Also histogram the NR energies we used so we can see where we probed Y\n",
    "\n",
    "#From python/damic_y.py\n",
    "import dataPython as dp #Anthony's text file library\n",
    "damic_data = dp.getXYdata_wXYerr('data/DAMIC_siyield_allerr.txt')\n",
    "#convert to numpy arrays\n",
    "damic_data['xx']= np.asarray(damic_data['xx'])*1000 #make units eV\n",
    "damic_data['yy']= np.asarray(damic_data['yy'])*1000 #make units eV\n",
    "damic_data['ex']= np.asarray(damic_data['ex'])*1000 #make units eV\n",
    "damic_data['ey']= np.asarray(damic_data['ey'])*1000 #make units eV\n",
    "\n",
    "#get the yield stuff\n",
    "damic_data['yy_yield'] = damic_data['yy']/damic_data['xx']\n",
    "damic_data['ey_yield'] = damic_data['yy_yield'] * np.sqrt((damic_data['ey']/damic_data['yy'])**2 + \n",
    "                                                          (damic_data['ex']/damic_data['xx'])**2)\n",
    "\n",
    "#Some Fit results\n",
    "popt_lind=[0.178, 1.619, 4.092]\n",
    "perr_lind=[4.9156e-04, 1.1264e-04, 2.8346e-03]\n",
    "\n",
    "popt_chav=[0.221, 0.003, 1.699, 4.389]\n",
    "perr_chav=[0.012, 0.001, 0.011, 0.108]\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(2,2,figsize=(12.0,8.0),sharex=True)\n",
    "Er_plt=np.linspace(0,3e3,1000)\n",
    "\n",
    "ax[0,0].errorbar(damic_data['xx'],damic_data['yy'],yerr=[damic_data['ey'], damic_data['ey']],\n",
    "               marker='o', linestyle='none', markersize=6, linewidth=2 ,label='Damic Points')\n",
    "\n",
    "ax[0,0].plot(Er_plt,Er_plt*yL(Er_plt,popt_lind[0]),label='Lindhard, k={0:.3f}'.format(popt_lind[0]))\n",
    "#ax[0,0].fill_between(Er_plt,Er_plt*yL(Er_plt,popt_lind[0]-perr_lind[0]),\n",
    "#                     Er_plt*yL(Er_plt,popt_lind[0]+perr_lind[0]),color='C1',alpha=0.5)\n",
    "\n",
    "ax[0,0].plot(Er_plt,Er_plt*yChav(Er_plt,popt_chav[0],popt_chav[1]),\n",
    "             label='Chavarria, k={0:.3f}, a={1:.3f}'.format(popt_chav[0],popt_chav[1]))\n",
    "\n",
    "ax[0,0].set_xlabel(r'$E_{recoil}$ [eV]')\n",
    "ax[0,0].set_ylim(0,1000)\n",
    "ax[0,0].set_ylabel(r'$E_{Ion}$ [eV]')\n",
    "ax[0,0].legend()\n",
    "\n",
    "ax[1,0].errorbar(damic_data['xx'],damic_data['yy_yield'],yerr=[damic_data['ey_yield'], damic_data['ey_yield']],\n",
    "               marker='o', linestyle='none', markersize=6, linewidth=2 ,label='Damic Points')\n",
    "ax[1,0].plot(Er_plt,yL(Er_plt,popt_lind[0]),label='Lindhard, k={0:.3f}'.format(popt_lind[0]))\n",
    "#ax[1,0].fill_between(Er_plt,yL(Er_plt,popt_lind[0]-perr_lind[0]),yL(Er_plt,popt_lind[0]+perr_lind[0]),color='C1',alpha=0.5)\n",
    "ax[1,0].plot(Er_plt,yChav(Er_plt,popt_chav[0],popt_chav[1]),label='Chavarria, k={0:.3f}, a={1:.3f}'.format(popt_chav[0],popt_chav[1]))\n",
    "\n",
    "ax[1,0].set_xlabel(r'$E_{recoil}$ [eV]')\n",
    "ax[1,0].set_ylabel('Y')\n",
    "\n",
    "\n",
    "Ebins=np.arange(1,3001,20)\n",
    "ax[0,1].hist(evec_nr_nocap.flatten(),alpha=0.5,bins=Ebins,label='Direct NR')\n",
    "ax[0,1].hist(cdata['E'][cdata['cEscape']].flatten(),alpha=0.5,bins=Ebins,label='E, (n,g)')\n",
    "ax[0,1].hist((cdata['E'][cdata['cEscape']]-cdata['delE'][cdata['cEscape']]).flatten(),alpha=0.5,bins=Ebins,label='E-dE, (n,g)')\n",
    "ax[0,1].set_yscale('log')\n",
    "ax[0,1].set_xlabel(r'$E_{recoil}$ [eV]')\n",
    "ax[0,1].set_ylabel('Counts')\n",
    "ax[0,1].legend()\n",
    "plt.tight_layout()\n",
    "#plt.savefig('figures/R68_Y_fit/Yfit_3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(evec_nr_nocap.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground\n",
    "Stuff below here is in alpha stage\n",
    "=========================================\n",
    "=========================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A redifinition of the fitting function for use with lmfit\n",
    "def FittingFunc_v2(params,x=None):\n",
    "    parvals = params.valuesdict()\n",
    "    Y = parvals['Y']\n",
    "    F_NR = parvals['F_NR']\n",
    "    fudge = parvals['fudge']\n",
    "    \n",
    "    E_bins, N_er, N_nr, N_ng, N_PuBe, N_bkg = buildSpectra(Y, F_NR, fudge)\n",
    "    \n",
    "    thresh_bin=5\n",
    "    \n",
    "    spec_meas = (N_PuBe-N_bkg)[thresh_bin:]\n",
    "    spec_pred = (N_er+N_nr+N_ng)[thresh_bin:]\n",
    "    unc_pred = np.sqrt(spec_pred)\n",
    "    \n",
    "    #return PoisLogLikelihood(spec_meas,spec_pred)\n",
    "    #return Chisq(spec_meas, spec_pred, unc_pred)\n",
    "    return ((spec_meas-spec_pred)**2)/(unc_pred**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with lmfit\n",
    "import lmfit\n",
    "\n",
    "# create a set of Parameters\n",
    "params = lmfit.Parameters()\n",
    "params.add('Y', value=0.8, min=0, max=1)\n",
    "params.add('F_NR', value=3, min=0, max=10)\n",
    "params.add('fudge', value=8e-3, vary=False, min=0, max=1e-1)\n",
    "\n",
    "# do fit, here with leastsq model\n",
    "minner = lmfit.Minimizer(FittingFunc_v2, params, epsfcn=0.1)\n",
    "result = minner.minimize()\n",
    "print(lmfit.fit_report(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integral Method\n",
    "As discussed in the Damic NR Yield paper, another approach is to compare the integrals of the simulated and measured spectra to obtain Y. This does not rely on a parameterized yield function. However, applying resolution and Fano effects are tricky.\n",
    "\n",
    "TODO: Where does Fano fit in here?\n",
    "\n",
    "How to account for multiple recoils in capture events?\n",
    "\n",
    "Steps\n",
    "1. Calculate normalization factor so Simulated ER spectrum agrees with Measured, bkg-subtracted spectrum above ~1.75 keV. We'll use the same factor for simulated NRs too. Somehow normalize (n,gamma) spectrum as well.\n",
    "2. Starting at the high energy end, count the number of measured events above electron-equivalent energy Ee_star, Ne_meas(Ee>Ee_star). Then find the nuclear recoil energy, Er_star such that: Ner_sim(Ee>Ee_star) + Nnr_sim(Er>Er_star)= Ne_meas(Ee>Ee_star). This will result in a set of points (Er_star,Ee_star) which define Y.\n",
    "3. Apply the calculated Y to the Simulated spectra, then smear it with the detector resolution, and use Y to convert back to nuclear recoil energies. Finally, repeat the steps in 2) to obtain Y from the smeared Simulated spectrum.\n",
    "4. Repeat the above steps, starting with the original unsmeared spectrum and the new Y.\n",
    "5. Continue this all until Y converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=Yield(evec_nr_nocap,[0.15,0.24,140])\n",
    "print(y)\n",
    "print(np.where(evec_nr_nocap>0,y,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Eos=np.linspace(0,1,10)\n",
    "s=1.0*((Eos>0.5) & (Eos<0.7))\n",
    "print(s,np.equal(s,0,dtype=float),(~np.equal(s,0,dtype=float))*s)\n",
    "print(gausInt(1,0,0.5,1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Try to figure out why the (n,gamma) calculations don't agree\n",
    "E_ng = cdata['E'][cdata['cEscape']]\n",
    "dE_ng = cdata['delE'][cdata['cEscape']]\n",
    "E_ng_er = E_ng*Yield(E_ng,[0.15]) - (E_ng-dE_ng)*Yield(E_ng-dE_ng,[0.15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The one-off way, many times for a single event\n",
    "#def getNeh(E,eps,F):\n",
    "#    N=np.random.normal((E/eps),np.sqrt(F*(E/eps)),np.shape(E))\n",
    "#    return np.maximum(0,N)\n",
    "\n",
    "Etest=E_ng[0]\n",
    "dE_test=dE_ng[0]\n",
    "Eer_test=E_ng_er[0]\n",
    "\n",
    "N_test = np.array([getNeh(Eer_test,eps,2.4) for i in range(100000)])\n",
    "\n",
    "Etv_test = N_test*V + dE_test\n",
    "Et_test = np.sum(Etv_test,1)\n",
    "\n",
    "#convert to eVee scale by making the gamma assumption\n",
    "Eee_test  = Et_test/G_NTL\n",
    "\n",
    "print(np.mean(N_test[:,0]),np.sum(Eer_test)/eps,np.std(N_test[:,0]),np.sqrt(2.4*Eer_test[0]/eps))\n",
    "print(np.mean(Eee_test),np.sum(dE_test + Eer_test/eps*V)/G_NTL,np.std(Eee_test),np.sqrt(np.sum(2.4*Eer_test/eps*V**2/((1+V/eps)**2))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now the average way\n",
    "AEeeVec_test = (dE_test + Eer_test*V/eps) / (1+V/eps)#Electon-equivalent\n",
    "AEeeMean_test = np.sum(AEeeVec_test)\n",
    "\n",
    "SigmaEeeSq_test = Eer_test*(V**2)*2.4/eps/((1 + V/eps)**2)\n",
    "SigmaEee_test = np.sqrt(np.sum(SigmaEeeSq_test))  \n",
    "\n",
    "print(SigmaEee_test)\n",
    "\n",
    "#Reshape so we can broadcast against bin centers\n",
    "#AEeeMean_test = AEeeMean_test[:,np.newaxis]\n",
    "#SigmaEee_test = SigmaEee_test[:,np.newaxis]\n",
    "\n",
    "#E_ee binning\n",
    "Eee_max = 500 #eVee\n",
    "Nbins_Eee = 50\n",
    "Eee_bins = np.linspace(0,Eee_max,Nbins_Eee+1)#bin edges\n",
    "\n",
    "#Weighted spectra of Eee energies measured\n",
    "Eee_counts_test = 100000*gausInt(AEeeMean_test, SigmaEee_test, Eee_bins[:-1], Eee_bins[1:])\n",
    "np.sum(gausInt(AEeeMean_test, SigmaEee_test, Eee_bins[:-1], Eee_bins[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(Eee_test,bins=Eee_bins)\n",
    "plt.step((Eee_bins[:-1]+Eee_bins[1:])/2,Eee_counts_test,where='mid', linestyle='-')\n",
    "#The rounding seems to make the difference in structure for 1 event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Eee_max = 2000 #eVee\n",
    "Nbins_Eee = 200\n",
    "Eee_bins = np.linspace(0,Eee_max,Nbins_Eee+1)#bin edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Eee_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py373_gammapi] *",
   "language": "python",
   "name": "conda-env-py373_gammapi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
