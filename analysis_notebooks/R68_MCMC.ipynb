{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R68 MCMC\n",
    "\n",
    "Use MCMC to estimate yield model parameters for R68 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../mplstyles\n",
      "3.1.1\n",
      "/data/chocula/mast/cdms/analysis/run068/R68_paper2019/mplstyles\n"
     ]
    }
   ],
   "source": [
    "#Set up notebook and load some R68 constants (V, eps, etc.)\n",
    "exec(open(\"nb_setup.py\").read())#Is there a better way to do this?\n",
    "from constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Measured Data...\n",
      "(480634,)\n",
      "(174510,)\n",
      "Loading Geant4 Data...\n",
      "(528848, 7)\n",
      "(129555, 7)\n",
      "Loading NRs...\n",
      "1.1  min\n",
      "Loading ERs...\n",
      "0.3  min\n",
      "Loading (n,gamma) Data...\n",
      "400000\n"
     ]
    }
   ],
   "source": [
    "#Load the data\n",
    "import R68_load as r68\n",
    "\n",
    "meas=r68.load_measured()\n",
    "g4=r68.load_G4(load_frac=0.1)\n",
    "cap=r68.load_simcap(file='/data/chocula/villaa/cascadeSimData/si28_R68_400k.pkl', rcapture=0.161, load_frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Energy binning\n",
    "Emax = 2000 #eVee\n",
    "Ebins=np.linspace(0,Emax,201)\n",
    "\n",
    "#Changed on 14 Feb 2020 for speed\n",
    "#Emax = 1000 #eVee \n",
    "#Ebins=np.linspace(0,Emax,101)\n",
    "#Ebin_ctr=(Ebins[:-1]+Ebins[1:])/2\n",
    "\n",
    "#Measured spectra\n",
    "N_meas_PuBe,_ = np.histogram(meas['PuBe']['E'],bins=Ebins)\n",
    "N_meas_Bkg,_ = np.histogram(meas['Bkg']['E'],bins=Ebins)\n",
    "\n",
    "tlive_PuBe = meas['PuBe']['tlive']\n",
    "tlive_Bkg = meas['Bkg']['tlive']\n",
    "#We'll scale everything to the PuBe live time and work with counts, not rate, to get the Poisson stats right\n",
    "\n",
    "N_meas_Bkg_scaled = N_meas_Bkg * tlive_PuBe/tlive_Bkg\n",
    "\n",
    "#Estimate of counts due to PuBe\n",
    "N_meas = N_meas_PuBe - N_meas_Bkg_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import yield models\n",
    "import R68_yield as Yield\n",
    "import R68_spec_tools as spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lind: Lindhard', 'Chav: Chavarria', 'Sor: Sorenson', 'Damic: Extrapolated Damic model']\n"
     ]
    }
   ],
   "source": [
    "Y=Yield.Yield('Lind',[0.15])\n",
    "print(Y.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define likelihood functions\n",
    "from scipy.special import factorial, gamma, loggamma\n",
    "\n",
    "#Poisson likelihood of measuring k given expected mean of lambda\n",
    "def pois_likelihood(k, lamb):\n",
    "    return (lamb**k)*np.exp(-lamb)/gamma(k+1.)\n",
    "\n",
    "#Poisson log-likelihood\n",
    "#k: observed counts\n",
    "#lamb: expected (model) counts\n",
    "def ll_pois(k, lamb):   \n",
    "    if np.sum(lamb<=0):\n",
    "        return -np.inf\n",
    "    \n",
    "    return np.sum(k*np.log(lamb) - lamb - loggamma(k+1.))\n",
    "\n",
    "#Normal log-likelihood, limit of Poisson for large lambda\n",
    "#k: observed counts\n",
    "#lamb: expected (model) counts\n",
    "def ll_norm(k,lamb):\n",
    "    if np.sum(lamb<=0):\n",
    "        return -np.inf\n",
    "    \n",
    "    return np.sum(-0.5*np.log(2*np.pi*lamb) - (k-lamb)**2/(2*lamb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log of flat prior functions\n",
    "#theta: array of parameter values\n",
    "#bounds: array of parameter bounds. shape should be len(theta)x2\n",
    "def lp_flat(theta, bounds):\n",
    "    #for itheta,ibounds in zip(theta,bounds):\n",
    "    #    if not (ibounds[0] < itheta < ibounds[1]):\n",
    "    #        return -np.inf\n",
    "        \n",
    "    #return 0.0\n",
    "    \n",
    "    if (np.array(bounds)[:,0]<theta).all() and (theta<np.array(bounds)[:,1]).all():\n",
    "        return 0.0\n",
    "    return -np.inf\n",
    "\n",
    "#Log of normal prior distribution\n",
    "#theta: parameter value(s)\n",
    "#mu: parameter prior distribution mean(s)\n",
    "#sigma: paramter prior distribution sigma(s)\n",
    "def lp_norm(theta, mu, sigma):\n",
    "    return np.sum(-0.5*((theta-mu)/sigma)**2 - np.log(sigma)-0.5*np.log(2*np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Log probability, log(likelihood*prior)\n",
    "#\n",
    "#model: pre-defined Yield model\n",
    "#theta: array of fit parameters (yield_par0, yield_par1, ...,  F_NR, scale_g4, scale_ng, ...)\n",
    "#theta_bounds: paramter bounds, shape should be len(theta)x2\n",
    "#spec_bounds: range of bin numbers in spectrum to consider. The analysis range is [bin_low,bin_high)\n",
    "#likelihood: Likelihood function, either 'Pois' or 'Norm'\n",
    "\n",
    "def calc_log_prob(model='Lind',theta=[0.2, 1, 1, 1], theta_bounds=((0,1),(0,10),(0,10),(0,10)), spec_bounds=(5,101),\n",
    "                  likelihood='Pois'):\n",
    "\n",
    "    #Access the global data we loaded\n",
    "    global N_meas, tlive_PuBe, g4, cap, Y\n",
    "    \n",
    "    ############\n",
    "    #Set some local variables\n",
    "    F_NR=None\n",
    "    scale_g4=None\n",
    "    scale_ng=None\n",
    "    NR=None\n",
    "    ER=None\n",
    "    NG=None\n",
    "    \n",
    "    if model=='Lind':\n",
    "        nYpar=1\n",
    "    elif (model=='Chav' or model=='Sor'):\n",
    "        nYpar=2\n",
    "    elif (model=='Damic'):\n",
    "        nYpar=0\n",
    "    else:\n",
    "        print('Error: Yield model not defined.')\n",
    "        return None\n",
    "    \n",
    "    Y.model=model\n",
    "    Y.pars=theta[:nYpar]\n",
    "    F_NR=theta[nYpar]\n",
    "    scale_g4=theta[nYpar+1]\n",
    "    scale_ng=theta[nYpar+2]\n",
    "    \n",
    "    \n",
    "    #Calculate the (log)prior first since we may not need to calculate the likelihood\n",
    "    lp=lp_flat(theta, theta_bounds)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "        \n",
    "    \n",
    "    ##########\n",
    "    #Build the spectra\n",
    "    #NR,ER,NG=spec.buildSimSpectra_ee(Ebins=Ebins, Evec_nr=g4['NR']['E'], Evec_er=g4['ER']['E'], Evec_ng=cap['E'], dEvec_ng=cap['dE'], \n",
    "                                         #Yield=Y, F_NR=F_NR, scale_g4=scale_g4, scale_ng=scale_ng, doDetRes=True, seed=1)\n",
    "\n",
    "    #Avg spectra is slower more stable\n",
    "    NR,ER,NG=spec.buildAvgSimSpectra_ee(Ebins=Ebins, Evec_nr=g4['NR']['E'], Evec_er=g4['ER']['E'], Evec_ng=cap['E'], dEvec_ng=cap['dE'],\n",
    "                                        Yield=Y, F_NR=F_NR, scale_g4=scale_g4, scale_ng=scale_ng, doDetRes=True, fpeak=1)\n",
    "\n",
    "\n",
    "    #Total counts for PuBe live time\n",
    "    #Uncertainty will be sqrt(N)\n",
    "    N_pred = (NR/g4['NR']['tlive'] + ER/g4['ER']['tlive'] + NG/cap['tlive'])*tlive_PuBe\n",
    "\n",
    "    ##########\n",
    "    #Calculate the log probability = log prior + log likelihood\n",
    "    ll=None\n",
    "    \n",
    "    if likelihood=='Norm':\n",
    "        ll = ll_norm(N_meas[slice(*spec_bounds)],N_pred[slice(*spec_bounds)])\n",
    "    elif likelihood=='Pois':\n",
    "        ll = ll_pois(N_meas[slice(*spec_bounds)],N_pred[slice(*spec_bounds)])\n",
    "    else:\n",
    "        print('Error: Bad likelihood')\n",
    "        return None\n",
    "    \n",
    "    if not np.isfinite(ll):\n",
    "        return -np.inf\n",
    "    \n",
    "    return lp + ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lindhard Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "from multiprocessing import Pool\n",
    "\n",
    "#Lindhard fit\n",
    "# theta = k, sim_scale, F_NR\n",
    "labels_l = ['k', 'sim scale', 'F_{NR}']\n",
    "\n",
    "Y=Yield.Yield('Lind',[0.2])\n",
    "\n",
    "#Wrapper help\n",
    "def LindFit_helper(theta):\n",
    "    return calc_log_prob(model='Lind', theta=theta, theta_bounds=((0,1),(0,10),(0,5)),\n",
    "                         spec_bounds=(5,101), likelihood='Pois')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwalkers_l = 8\n",
    "ndim_l = 3\n",
    "nstep_l=5000\n",
    "#guesses_l = np.array([0.18, 1.6, 3.8]) + 1e-3 * np.random.randn(nwalkers_l, ndim_l)\n",
    "guesses_l = np.array([0.18, 1.6, 3.8]) + np.array([1e-2, 1, 0.5]) * np.random.randn(nwalkers_l, ndim_l)\n",
    "\n",
    "with Pool(processes=20) as pool: #Can fail depending on current memory usage of other processes...\n",
    "    sampler_l = emcee.EnsembleSampler(nwalkers_l, ndim_l, LindFit_helper, pool=pool)\n",
    "    sampler_l.run_mcmc(guesses_l, nstep_l, progress=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save this work\n",
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "saveMCMC=True\n",
    "\n",
    "if saveMCMC:\n",
    "    ifile = 0\n",
    "    fname='data/mcmc_Lind_{0}walk_{1}step_pois_v{2}.pkl'.format(nwalkers_l,nstep_l,ifile+1)\n",
    "    while os.path.exists(fname):\n",
    "        ifile += 1\n",
    "        fname='data/mcmc_Lind_{0}walk_{1}step_pois_v{2}.pkl'.format(nwalkers_l,nstep_l,ifile+1)\n",
    "        \n",
    "    print(fname)\n",
    "    saveFile = open(fname, 'wb')\n",
    "    pkl.dump(sampler_l,saveFile)\n",
    "    pkl.dump(guesses_l,saveFile)\n",
    "    pkl.dump(labels_l,saveFile)\n",
    "    saveFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the chain of parameter values\n",
    "fig, axes = plt.subplots(ndim_l, figsize=(10, 7), sharex=True)\n",
    "samples_l = sampler_l.get_chain()\n",
    "for i in range(ndim_l):\n",
    "    ax = axes[i]\n",
    "    ax.plot(samples_l[:, :, i], \"k\", alpha=0.3)\n",
    "    ax.set_xlim(0, len(samples_l))\n",
    "    #ax.set_ylim(0, 5)\n",
    "    ax.set_ylabel(labels_l[i])\n",
    "    ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "\n",
    "axes[-1].set_xlabel(\"step number\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the sample autocorrelation time\n",
    "tau_l=sampler_l.get_autocorr_time()\n",
    "print(tau_l)\n",
    "avgtau_l=round(np.average(tau_l))\n",
    "print(avgtau_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discard a few times tau as burn-in and thin by tau/2\n",
    "avgtau_l=90\n",
    "flat_samples_l = sampler_l.get_chain(discard=int(2.*avgtau_l), thin=int(round(avgtau_l/2.)), flat=True)\n",
    "#flat_samples_l = sampler.get_chain(discard=40, flat=True)\n",
    "print(flat_samples_l.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "fig = corner.corner(flat_samples_l, labels=labels_l, quantiles=[0.16, 0.5, 0.84], show_titles=True, title_fmt='0.3f');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Math\n",
    "\n",
    "for i in range(ndim_l):\n",
    "    mcmc = np.percentile(flat_samples_l[:, i], [16, 50, 84])\n",
    "    q = np.diff(mcmc)\n",
    "    txt = \"\\mathrm{{{3}}} = {0:.3f}_{{-{1:.3f}}}^{{+{2:.3f}}}\"\n",
    "    txt = txt.format(mcmc[1], q[0], q[1], labels_l[i])\n",
    "    display(Math(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flat_samples_l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot best fit yield\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "Er_plot=np.linspace(100,3e3,100)\n",
    "\n",
    "inds = np.random.randint(len(flat_samples_l), size=100)\n",
    "for ind in inds:\n",
    "    thetai = flat_samples_l[ind][0]\n",
    "    Y=Yield.Yield('Lind',[thetai])\n",
    "    plt.plot(Er_plot, Y.calc(Er_plot), \"C3\", alpha=0.1)\n",
    "\n",
    "ax.set_xlabel('Er [eV]')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_ylim(0.1,0.3);\n",
    "\n",
    "#plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chavarria Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "from multiprocessing import Pool\n",
    "\n",
    "#Chavarria fit\n",
    "# theta = k, a, sim_scale, F_NR\n",
    "\n",
    "# k: Lindhard k factor [unitless]\n",
    "# ainv: Inverse Chavaria cutoff factor [1/eV]\n",
    "#labels_c = ['k', 'ainv', 'sim scale', 'F_{NR}']\n",
    "\n",
    "Y=Yield.Yield('Chav',[0.2,10])\n",
    "\n",
    "#Wrapper help\n",
    "#def ChavFit_helper(theta):\n",
    "#    return calc_log_prob(model='Chav', theta=theta, theta_bounds=((0,1),(0,100),(0,10),(0,5)),\n",
    "#                         spec_bounds=(5,101), likelihood='Pois')\n",
    "\n",
    "\n",
    "# theta = k, a, F_NR, scale_g4, scale_ng,\n",
    "labels_c = [r'k', r'$a^{-1}$', r'$F_{NR}$', r'$scale_{G4}$', r'$scale_{ng}$']\n",
    "\n",
    "#Test full range fit\n",
    "def ChavFit_helper(theta):\n",
    "    return calc_log_prob(model='Chav', theta=theta, theta_bounds=((0,1),(0,1000),(0,10),(0,10),(0,10)),\n",
    "                         spec_bounds=(5,201), likelihood='Pois')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nwalkers_c = 8\n",
    "#ndim_c = 4\n",
    "#nstep_c = 5000\n",
    "\n",
    "#guesses_c = np.array([0.18, 10., 1.6, 3.8]) + np.array([1e-2, 1., 1, 0.5]) * np.random.randn(nwalkers_c, ndim_c)\n",
    "\n",
    "#with Pool(processes=20) as pool: #Can fail depending on current memory usage of other processes...\n",
    "#    sampler_c = emcee.EnsembleSampler(nwalkers_c, ndim_c, ChavFit_helper, pool=pool)\n",
    "#    sampler_c.run_mcmc(guesses_c, nstep_c, progress=True);\n",
    "    \n",
    "nwalkers_c = 16\n",
    "ndim_c = 5\n",
    "nstep_c = 5000\n",
    "\n",
    "guesses_c = np.array([0.18, 10, 3.0, 1.0, 1.0]) + np.array([1e-2, 1, 1, 0.1, 0.1]) * np.random.randn(nwalkers_c, ndim_c)\n",
    "\n",
    "with Pool(processes=20) as pool: #Can fail depending on current memory usage of other processes...\n",
    "    sampler_c = emcee.EnsembleSampler(nwalkers_c, ndim_c, ChavFit_helper, pool=pool)\n",
    "    sampler_c.run_mcmc(guesses_c, nstep_c, progress=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save this work\n",
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "saveMCMC=True\n",
    "\n",
    "if saveMCMC:\n",
    "    ifile = 0\n",
    "    fname='data/mcmc_Chav_{0}walk_{1}step_pois_v{2}.pkl'.format(nwalkers_c,nstep_c,ifile+1)\n",
    "    while os.path.exists(fname):\n",
    "        ifile += 1\n",
    "        fname='data/mcmc_Chav_{0}walk_{1}step_pois_v{2}.pkl'.format(nwalkers_c,nstep_c,ifile+1)\n",
    "        \n",
    "    print(fname)\n",
    "    saveFile = open(fname, 'wb')\n",
    "    \n",
    "    results={'sampler':sampler_c, 'guesses': guesses_c, 'labels':labels_c}\n",
    "    \n",
    "    pkl.dump(results,saveFile)\n",
    "    saveFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorenson Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "from multiprocessing import Pool\n",
    "\n",
    "#Sorenson fit\n",
    "# theta = k, q, sim_scale, F_NR\n",
    "# k: Lindhard k factor [unitless]\n",
    "# q: Cutoff energy, in units of Lindhard epsilon (eps = 11.5*Er/1000*Z**(-7./3))\n",
    "#labels_s = ['k', 'q', 'sim scale', 'F_{NR}']\n",
    "\n",
    "Y=Yield.Yield('Sor',[0.2,2e-3])\n",
    "\n",
    "#Wrapper help\n",
    "#def SorFit_helper(theta):\n",
    "#    return calc_log_prob(model='Sor', theta=theta, theta_bounds=((0,1),(0,3e-2),(0,10),(0,5)),\n",
    "#                         spec_bounds=(5,101), likelihood='Pois')\n",
    "\n",
    "# theta = k, q, F_NR, scale_g4, scale_ng,\n",
    "labels_s = [r'k', r'q', r'$F_{NR}$', r'$scale_{G4}$', r'$scale_{ng}$']\n",
    "\n",
    "#Wrapper help\n",
    "#def SorFit_helper(theta):\n",
    "#    return calc_log_prob(model='Sor', theta=theta, theta_bounds=((0,1),(0,3e-2),(0,10),(0,10),(0,10)),\n",
    "#                         spec_bounds=(5,101), likelihood='Pois')\n",
    "\n",
    "#Test full range fit\n",
    "def SorFit_helper(theta):\n",
    "    E_lim_min=50 #eVee\n",
    "    E_lim_max=1.75e3 #eVee\n",
    "    spec_bounds=(np.digitize(E_lim_min,Ebins)-1,np.digitize(E_lim_max,Ebins)-1)\n",
    "                 \n",
    "    return calc_log_prob(model='Sor', theta=theta, theta_bounds=((0,1),(0,3e-2),(0,10),(0,10),(0,10)),\n",
    "                         spec_bounds=spec_bounds, likelihood='Pois')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [5:39:18<00:00,  4.07s/it]  \n"
     ]
    }
   ],
   "source": [
    "#nwalkers_s = 8\n",
    "#ndim_s = 4\n",
    "#nstep_s = 5000\n",
    "\n",
    "#guesses_s = np.array([0.18, 2e-3, 1.6, 3.8]) + np.array([1e-2, 1e-4, 1, 0.5]) * np.random.randn(nwalkers_s, ndim_s)\n",
    "\n",
    "nwalkers_s = 16\n",
    "ndim_s = 5\n",
    "nstep_s = 5000\n",
    "\n",
    "#guesses_s = np.array([0.18, 2e-3, 3.0, 1.0, 1.0]) + np.array([1e-2, 1e-4, 1, 0.1, 0.1]) * np.random.randn(nwalkers_s, ndim_s)\n",
    "\n",
    "#Sample priors uniformly\n",
    "tbs=np.array(((0,1),(0,3e-2),(0,10),(0,10),(0,10)))\n",
    "guesses_s=(np.array(tbs)[:,1]-np.array(tbs)[:,0])*np.random.random_sample((nwalkers_s, ndim_s))+np.array(tbs)[:,0]\n",
    "\n",
    "with Pool(processes=20) as pool: #Can fail depending on current memory usage of other processes...\n",
    "    sampler_s = emcee.EnsembleSampler(nwalkers_s, ndim_s, SorFit_helper, pool=pool)\n",
    "    sampler_s.run_mcmc(guesses_s, nstep_s, progress=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/mcmc_Sor_16walk_5000step_pois_v9.pkl\n"
     ]
    }
   ],
   "source": [
    "#Save this work\n",
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "saveMCMC=True\n",
    "\n",
    "if saveMCMC:\n",
    "    ifile = 0\n",
    "    fname='data/mcmc_Sor_{0}walk_{1}step_pois_v{2}.pkl'.format(nwalkers_s,nstep_s,ifile+1)\n",
    "    while os.path.exists(fname):\n",
    "        ifile += 1\n",
    "        fname='data/mcmc_Sor_{0}walk_{1}step_pois_v{2}.pkl'.format(nwalkers_s,nstep_s,ifile+1)\n",
    "        \n",
    "    print(fname)\n",
    "    saveFile = open(fname, 'wb')\n",
    "    \n",
    "    results={'sampler':sampler_s, 'guesses': guesses_s, 'labels':labels_s}\n",
    "    \n",
    "    pkl.dump(results,saveFile)\n",
    "    saveFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the chain of parameter values\n",
    "fig, axes = plt.subplots(ndim_s, figsize=(10, 7), sharex=True)\n",
    "samples_s = sampler_s.get_chain()\n",
    "for i in range(ndim_s):\n",
    "    ax = axes[i]\n",
    "    ax.plot(samples_s[:, :, i], \"k\", alpha=0.3)\n",
    "    ax.set_xlim(0, len(samples_s))\n",
    "    #ax.set_ylim(0, 5)\n",
    "    ax.set_ylabel(labels_s[i])\n",
    "    ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "\n",
    "axes[-1].set_xlabel(\"step number\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the sample autocorrelation time\n",
    "tau_s=sampler_s.get_autocorr_time()\n",
    "print(tau_s)\n",
    "avgtau_s=round(np.average(tau_s))\n",
    "print(avgtau_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discard a few times tau as burn-in and thin by tau/2\n",
    "avgtau_s=200\n",
    "flat_samples_s = sampler_s.get_chain(discard=int(2.*avgtau_s), thin=int(round(avgtau_s/2.)), flat=True)\n",
    "print(flat_samples_s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "fig = corner.corner(flat_samples_s, labels=labels_s, quantiles=[0.16, 0.5, 0.84], show_titles=True, title_fmt='0.3f');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lind Fit Including Systematics\n",
    "Let's try to include nuisance factors for things like measured energy calibration and cut efficiencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Log probability, log(likelihood*prior)\n",
    "#\n",
    "#model: pre-defined Yield model\n",
    "#theta: array of fit parameters: here its (yield pars, sim scale, F_NR, calibration scale)\n",
    "#theta_bounds: paramter bounds, shape should be len(theta)x2\n",
    "#spec_bounds: range of bin numbers in spectrum to consider. The analysis range is [bin_low,bin_high)\n",
    "#likelihood: Likelihood function, either 'Pois' or 'Norm'\n",
    "\n",
    "def calc_log_prob_wnuis(model='Lind',theta=[0.2, 1.5, 2.4, 1.0], theta_bounds=((0,1),(0,10),(0,5),(0.5,1.5)), \n",
    "                        spec_bounds=(5,101), likelihood='Pois'):\n",
    "\n",
    "    #Access the global data we loaded\n",
    "    global meas, tlive_PuBe, g4, cap, Y\n",
    "    \n",
    "    ############\n",
    "    #Set some local variables\n",
    "    F_NR=None\n",
    "    scale_g4=None\n",
    "    scale_ng=None\n",
    "    NR=None\n",
    "    ER=None\n",
    "    NG=None\n",
    "    \n",
    "    if model=='Lind':\n",
    "        nYpar=1\n",
    "    elif (model=='Chav' or model=='Sor'):\n",
    "        nYpar=2\n",
    "    elif (model=='Damic'):\n",
    "        nYpar=0\n",
    "    else:\n",
    "        print('Error: Yield model not defined.')\n",
    "        return None\n",
    "    \n",
    "    Y.model=model\n",
    "    Y.pars=theta[:nYpar]\n",
    "    F_NR=theta[nYpar+1]\n",
    "    scale_g4=theta[nYpar]\n",
    "    scale_ng=scale_g4\n",
    "    cal_scale=theta[nYpar+2]\n",
    "    \n",
    "    ##########\n",
    "    #Build the measured spectra    \n",
    "    N_meas_PuBe,_ = np.histogram(cal_scale*meas['PuBe']['E'],bins=Ebins)\n",
    "    N_meas_Bkg,_ = np.histogram(cal_scale*meas['Bkg']['E'],bins=Ebins)\n",
    "\n",
    "    tlive_PuBe = meas['PuBe']['tlive']\n",
    "    tlive_Bkg = meas['Bkg']['tlive']\n",
    "    \n",
    "    #We'll scale everything to the PuBe live time and work with counts, not rate, to get the Poisson stats right\n",
    "    N_meas_Bkg_scaled = N_meas_Bkg * tlive_PuBe/tlive_Bkg\n",
    "\n",
    "    #Estimate of counts due to PuBe\n",
    "    N_meas = N_meas_PuBe - N_meas_Bkg_scaled\n",
    "    \n",
    "    \n",
    "    #Calculate the (log)prior first since we may not need to calculate the likelihood\n",
    "    lp=lp_flat(theta[:nYpar+2], theta_bounds[:nYpar+2])\n",
    "    lp+=lp_norm(theta[nYpar+2], 1, 0.1)#normal distribution for cal_scale\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "        \n",
    "    \n",
    "    ##########\n",
    "    #Build the simulated spectra\n",
    "    #Avg spectra is more stable\n",
    "    NR,ER,NG=spec.buildAvgSimSpectra_ee(Ebins=Ebins, Evec_nr=g4['NR']['E'], Evec_er=g4['ER']['E'], Evec_ng=cap['E'], dEvec_ng=cap['dE'],\n",
    "                                        Yield=Y, F_NR=F_NR, scale_g4=scale_g4, scale_ng=scale_ng, doDetRes=True)\n",
    "\n",
    "\n",
    "    #Total counts for PuBe live time\n",
    "    #Uncertainty will be sqrt(N)\n",
    "    N_pred = (NR/g4['NR']['tlive'] + ER/g4['ER']['tlive'] + NG/cap['tlive'])*tlive_PuBe\n",
    "\n",
    "    ##########\n",
    "    #Calculate the log probability = log prior + log likelihood\n",
    "    ll=None\n",
    "    \n",
    "    if likelihood=='Norm':\n",
    "        ll = ll_norm(N_meas[slice(*spec_bounds)],N_pred[slice(*spec_bounds)])\n",
    "    elif likelihood=='Pois':\n",
    "        ll = ll_pois(N_meas[slice(*spec_bounds)],N_pred[slice(*spec_bounds)])\n",
    "    else:\n",
    "        print('Error: Bad likelihood')\n",
    "        return None\n",
    "    \n",
    "    if not np.isfinite(ll):\n",
    "        return -np.inf\n",
    "    \n",
    "    return lp + ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "from multiprocessing import Pool\n",
    "\n",
    "#Lindhard fit\n",
    "# theta = k, sim_scale, F_NR, cal_scale\n",
    "labels_l = ['k', 'sim scale', 'F_{NR}', 'cal scale']\n",
    "\n",
    "Y=Yield.Yield('Lind',[0.2])\n",
    "\n",
    "#Wrapper help\n",
    "def LindFit_wnuis_helper(theta):\n",
    "    return calc_log_prob_wnuis(model='Lind', theta=theta, theta_bounds=((0,1),(0,10),(0,5),(0,10)),\n",
    "                         spec_bounds=(5,101), likelihood='Pois')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwalkers_l = 8\n",
    "ndim_l = 4\n",
    "nstep_l=5000\n",
    "#guesses_l = np.array([0.18, 1.6, 3.8, 1.0]) + 1e-3 * np.random.randn(nwalkers_l, ndim_l)\n",
    "guesses_l = np.array([0.18, 1.6, 3.8, 1.0]) + np.array([1e-2, 1, 0.5, 1e-1]) * np.random.randn(nwalkers_l, ndim_l) #Set the randoms scale better\n",
    "\n",
    "with Pool(processes=20) as pool: #Can fail depending on current memory usage of other processes...\n",
    "    sampler_l = emcee.EnsembleSampler(nwalkers_l, ndim_l, LindFit_wnuis_helper, pool=pool)\n",
    "    sampler_l.run_mcmc(guesses_l, nstep_l, progress=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save this work\n",
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "saveMCMC=True\n",
    "\n",
    "if saveMCMC:\n",
    "    ifile = 0\n",
    "    fname='data/mcmc_Lind_wcalscale_{0}walk_{1}step_pois_v{2}.pkl'.format(nwalkers_l,nstep_l,ifile+1)\n",
    "    while os.path.exists(fname):\n",
    "        ifile += 1\n",
    "        fname='data/mcmc_Lind_wcalscale_{0}walk_{1}step_pois_v{2}.pkl'.format(nwalkers_l,nstep_l,ifile+1)\n",
    "        \n",
    "    print(fname)\n",
    "    saveFile = open(fname, 'wb')\n",
    "    pkl.dump(sampler_l,saveFile)\n",
    "    pkl.dump(guesses_l,saveFile)\n",
    "    saveFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the chain of parameter values\n",
    "fig, axes = plt.subplots(ndim_l, figsize=(10, 7), sharex=True)\n",
    "samples_l = sampler_l.get_chain()\n",
    "for i in range(ndim_l):\n",
    "    ax = axes[i]\n",
    "    ax.plot(samples_l[:, :, i], \"k\", alpha=0.3)\n",
    "    ax.set_xlim(0, len(samples_l))\n",
    "    #ax.set_ylim(0, 5)\n",
    "    ax.set_ylabel(labels_l[i])\n",
    "    ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "\n",
    "axes[-1].set_xlabel(\"step number\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the sample autocorrelation time\n",
    "tau_l=sampler_l.get_autocorr_time()\n",
    "print(tau_l)\n",
    "avgtau_l=round(np.average(tau_l))\n",
    "print(avgtau_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discard a few times tau as burn-in and thin by tau/2\n",
    "avgtau_l=150\n",
    "flat_samples_l = sampler_l.get_chain(discard=int(2.*avgtau_l), thin=int(round(avgtau_l/2.)), flat=True)\n",
    "#flat_samples_l = sampler.get_chain(discard=40, flat=True)\n",
    "print(flat_samples_l.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "\n",
    "fig = corner.corner(\n",
    "    flat_samples_l, labels=labels_l\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(samples_l[:, :, 0].flatten(),samples_l[:, :, 3].flatten(),'.',alpha=0.01)\n",
    "plt.gca().set_ylim(0.9990,0.9995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nr_fano] *",
   "language": "python",
   "name": "conda-env-nr_fano-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
