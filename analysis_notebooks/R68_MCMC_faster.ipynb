{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R68 MCMC_faster\n",
    "\n",
    "Use MCMC to estimate yield model parameters for R68 data.\n",
    "This is a redesigned version of the R68_MCMC notebook.\n",
    "It is an effort to streamline the calculations and keep better track of settings used in each mcmc fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "#Set up notebook and load some R68 constants (V, eps, etc.)\n",
    "exec(open(\"nb_setup.py\").read())#Is there a better way to do this?\n",
    "from constants import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a dictionary to store all the MCMC fit parameters and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_data={'g4_load_frac':0.1,\n",
    "          'cap_load_frac':0.1,\n",
    "          'cap_sim_file':'/data/chocula/villaa/cascadeSimData/si28_R68_400k.pkl',\n",
    "          'cap_rcapture':0.161,\n",
    "          'Emax':None,\n",
    "          'Ebins':None,\n",
    "           'Ymodel':'Lind',\n",
    "           'likelihood':'Pois'\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Measured Data...\n",
      "(480634,)\n",
      "(174510,)\n",
      "Loading Geant4 Data...\n",
      "(528848, 7)\n",
      "(129555, 7)\n",
      "Loading NRs...\n",
      "1.1  min\n",
      "Loading ERs...\n",
      "0.3  min\n",
      "Loading (n,gamma) Data...\n",
      "400000\n"
     ]
    }
   ],
   "source": [
    "#Load the datasets\n",
    "import R68_load as r68\n",
    "\n",
    "meas=r68.load_measured()\n",
    "g4=r68.load_G4(load_frac=mcmc_data['g4_load_frac'])\n",
    "cap=r68.load_simcap(file=mcmc_data['cap_sim_file'], rcapture=mcmc_data['cap_rcapture'], load_frac=mcmc_data['cap_load_frac'])\n",
    "\n",
    "#Import yield models\n",
    "import R68_yield as Yield\n",
    "import R68_spec_tools as spec\n",
    "\n",
    "Y=Yield.Yield('Lind',[0.15])\n",
    "print(Y.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define likelihood functions\n",
    "from scipy.special import factorial, gamma, loggamma\n",
    "\n",
    "#Poisson likelihood of measuring k given expected mean of lambda\n",
    "def pois_likelihood(k, lamb):\n",
    "    return (lamb**k)*np.exp(-lamb)/gamma(k+1.)\n",
    "\n",
    "#Poisson log-likelihood\n",
    "#k: observed counts\n",
    "#lamb: expected (model) counts\n",
    "def ll_pois(k, lamb):   \n",
    "    if np.sum(lamb<=0):\n",
    "        return -np.inf\n",
    "    \n",
    "    return np.sum(k*np.log(lamb) - lamb - loggamma(k+1.))\n",
    "\n",
    "#Normal log-likelihood, limit of Poisson for large lambda\n",
    "#k: observed counts\n",
    "#lamb: expected (model) counts\n",
    "def ll_norm(k,lamb):\n",
    "    if np.sum(lamb<=0):\n",
    "        return -np.inf\n",
    "    \n",
    "    return np.sum(-0.5*np.log(2*np.pi*lamb) - (k-lamb)**2/(2*lamb))\n",
    "\n",
    "#Log of flat prior functions\n",
    "#theta: array of parameter values\n",
    "#bounds: array of parameter bounds. shape should be len(theta)x2\n",
    "def lp_flat(theta, bounds):\n",
    "    #for itheta,ibounds in zip(theta,bounds):\n",
    "    #    if not (ibounds[0] < itheta < ibounds[1]):\n",
    "    #        return -np.inf\n",
    "        \n",
    "    #return 0.0\n",
    "    \n",
    "    if (np.array(bounds)[:,0]<theta).all() and (theta<np.array(bounds)[:,1]).all():\n",
    "        return 0.0\n",
    "    return -np.inf\n",
    "\n",
    "#Log of normal prior distribution\n",
    "#theta: parameter value(s)\n",
    "#mu: parameter prior distribution mean(s)\n",
    "#sigma: paramter prior distribution sigma(s)\n",
    "def lp_norm(theta, mu, sigma):\n",
    "    return np.sum(-0.5*((theta-mu)/sigma)**2 - np.log(sigma)-0.5*np.log(2*np.pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the probability function. This contains much of the meat of this calculation.\n",
    "It takes our model parameters and returns the resulting probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Log probability, log(likelihood*prior)\n",
    "#\n",
    "#theta: array of fit parameters (yield_par0, yield_par1, ...,  F_NR, scale_g4, scale_ng, ...)\n",
    "#theta_bounds: paramter bounds, shape should be len(theta)x2\n",
    "#spec_bounds: range of bin numbers in spectrum to consider. The analysis range is [bin_low,bin_high)\n",
    "#likelihood: Likelihood function, either 'Pois' or 'Norm'\n",
    "\n",
    "def calc_log_prob(theta=[0.2, 1, 1, 1], theta_bounds=((0,1),(0,10),(0,10),(0,10)), spec_bounds=(5,101),\n",
    "                  likelihood='Pois'):\n",
    "\n",
    "    #Access the global data\n",
    "    #These must be already defined!!!\n",
    "    global N_meas, tlive_PuBe, g4, cap, Y\n",
    "    \n",
    "    ############\n",
    "    #Set some local variables\n",
    "    nYpar=Y.npars\n",
    "    \n",
    "    Y.set_model(model)\n",
    "    Y.set_pars(theta[:nYpar])\n",
    "    F_NR=theta[nYpar]\n",
    "    scale_g4=theta[nYpar+1]\n",
    "    scale_ng=theta[nYpar+2]\n",
    "    \n",
    "    \n",
    "    #Calculate the (log)prior first since we may not need to calculate the likelihood\n",
    "    lp=lp_flat(theta, theta_bounds)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "        \n",
    "    \n",
    "    ##########\n",
    "    #Build the spectra\n",
    "    #NR,ER,NG=spec.buildSimSpectra_ee(Ebins=Ebins, Evec_nr=g4['NR']['E'], Evec_er=g4['ER']['E'], Evec_ng=cap['E'], dEvec_ng=cap['dE'], \n",
    "                                         #Yield=Y, F_NR=F_NR, scale_g4=scale_g4, scale_ng=scale_ng, doDetRes=True, seed=1)\n",
    "\n",
    "    #Avg spectra is slower, but more stable\n",
    "    NR,ER,NG=spec.buildAvgSimSpectra_ee(Ebins=Ebins, Evec_nr=g4['NR']['E'], Evec_er=g4['ER']['E'], Evec_ng=cap['E'], dEvec_ng=cap['dE'],\n",
    "                                        Yield=Y, F_NR=F_NR, scale_g4=scale_g4, scale_ng=scale_ng, doDetRes=True, fpeak=1)\n",
    "\n",
    "\n",
    "    #Total counts for PuBe live time\n",
    "    #Uncertainty will be sqrt(N)\n",
    "    N_pred = (NR/g4['NR']['tlive'] + ER/g4['ER']['tlive'] + NG/cap['tlive'])*tlive_PuBe\n",
    "\n",
    "    ##########\n",
    "    #Calculate the log probability = log prior + log likelihood\n",
    "    ll=None\n",
    "    \n",
    "    if likelihood=='Norm':\n",
    "        ll = ll_norm(N_meas[slice(*spec_bounds)],N_pred[slice(*spec_bounds)])\n",
    "    elif likelihood=='Pois':\n",
    "        ll = ll_pois(N_meas[slice(*spec_bounds)],N_pred[slice(*spec_bounds)])\n",
    "    else:\n",
    "        print('Error: Bad likelihood')\n",
    "        return None\n",
    "    \n",
    "    if not np.isfinite(ll):\n",
    "        return -np.inf\n",
    "    \n",
    "    return lp + ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set eVee energy binning\n",
    "Emax = 2000 #eVee\n",
    "Ebins=np.linspace(0,Emax,201)\n",
    "mcmc_data['Emax']=Emax\n",
    "mcmc_data['Ebins']=Ebins\n",
    "\n",
    "\n",
    "#Measured spectra\n",
    "N_meas_PuBe,_ = np.histogram(meas['PuBe']['E'],bins=Ebins)\n",
    "N_meas_Bkg,_ = np.histogram(meas['Bkg']['E'],bins=Ebins)\n",
    "\n",
    "tlive_PuBe = meas['PuBe']['tlive']\n",
    "tlive_Bkg = meas['Bkg']['tlive']\n",
    "#We'll scale everything to the PuBe live time and work with counts, not rate, to get the Poisson stats right\n",
    "\n",
    "N_meas_Bkg_scaled = N_meas_Bkg * tlive_PuBe/tlive_Bkg\n",
    "\n",
    "#Estimate of counts due to PuBe\n",
    "N_meas = N_meas_PuBe - N_meas_Bkg_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorenson Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "from multiprocessing import Pool\n",
    "\n",
    "#Sorenson fit\n",
    "# theta = k, q, sim_scale, F_NR\n",
    "# k: Lindhard k factor [unitless]\n",
    "# q: Cutoff energy, in units of Lindhard epsilon (eps = 11.5*Er/1000*Z**(-7./3))\n",
    "#labels_s = ['k', 'q', 'sim scale', 'F_{NR}']\n",
    "\n",
    "Y=Yield.Yield('Sor',[0.2,2e-3])\n",
    "\n",
    "#Wrapper help\n",
    "#def SorFit_helper(theta):\n",
    "#    return calc_log_prob(model='Sor', theta=theta, theta_bounds=((0,1),(0,3e-2),(0,10),(0,5)),\n",
    "#                         spec_bounds=(5,101), likelihood='Pois')\n",
    "\n",
    "# theta = k, q, F_NR, scale_g4, scale_ng,\n",
    "labels_s = [r'k', r'q', r'$F_{NR}$', r'$scale_{G4}$', r'$scale_{ng}$']\n",
    "\n",
    "#Wrapper help\n",
    "#def SorFit_helper(theta):\n",
    "#    return calc_log_prob(model='Sor', theta=theta, theta_bounds=((0,1),(0,3e-2),(0,10),(0,10),(0,10)),\n",
    "#                         spec_bounds=(5,101), likelihood='Pois')\n",
    "\n",
    "#Test full range fit\n",
    "def SorFit_helper(theta):\n",
    "    E_lim_min=50 #eVee\n",
    "    E_lim_max=1.75e3 #eVee\n",
    "    spec_bounds=(np.digitize(E_lim_min,Ebins)-1,np.digitize(E_lim_max,Ebins)-1)\n",
    "                 \n",
    "    return calc_log_prob(model='Sor', theta=theta, theta_bounds=((0,1),(0,3e-2),(0,10),(0,10),(0,10)),\n",
    "                         spec_bounds=spec_bounds, likelihood='Pois')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 3994/5000 [4:31:12<1:12:21,  4.32s/it]"
     ]
    }
   ],
   "source": [
    "#nwalkers_s = 8\n",
    "#ndim_s = 4\n",
    "#nstep_s = 5000\n",
    "\n",
    "#guesses_s = np.array([0.18, 2e-3, 1.6, 3.8]) + np.array([1e-2, 1e-4, 1, 0.5]) * np.random.randn(nwalkers_s, ndim_s)\n",
    "\n",
    "nwalkers_s = 16\n",
    "ndim_s = 5\n",
    "nstep_s = 5000\n",
    "\n",
    "#guesses_s = np.array([0.18, 2e-3, 3.0, 1.0, 1.0]) + np.array([1e-2, 1e-4, 1, 0.1, 0.1]) * np.random.randn(nwalkers_s, ndim_s)\n",
    "\n",
    "#Sample priors uniformly\n",
    "tbs=np.array(((0,1),(0,3e-2),(0,10),(0,10),(0,10)))\n",
    "guesses_s=(np.array(tbs)[:,1]-np.array(tbs)[:,0])*np.random.random_sample((nwalkers_s, ndim_s))+np.array(tbs)[:,0]\n",
    "\n",
    "with Pool(processes=20) as pool: #Can fail depending on current memory usage of other processes...\n",
    "    sampler_s = emcee.EnsembleSampler(nwalkers_s, ndim_s, SorFit_helper, pool=pool)\n",
    "    sampler_s.run_mcmc(guesses_s, nstep_s, progress=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save this work\n",
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "saveMCMC=True\n",
    "\n",
    "if saveMCMC:\n",
    "    ifile = 0\n",
    "    fname='data/mcmc_Sor_{0}walk_{1}step_pois_v{2}.pkl'.format(nwalkers_s,nstep_s,ifile+1)\n",
    "    while os.path.exists(fname):\n",
    "        ifile += 1\n",
    "        fname='data/mcmc_Sor_{0}walk_{1}step_pois_v{2}.pkl'.format(nwalkers_s,nstep_s,ifile+1)\n",
    "        \n",
    "    print(fname)\n",
    "    saveFile = open(fname, 'wb')\n",
    "    \n",
    "    results={'sampler':sampler_s, 'guesses': guesses_s, 'labels':labels_s}\n",
    "    \n",
    "    pkl.dump(results,saveFile)\n",
    "    saveFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the chain of parameter values\n",
    "fig, axes = plt.subplots(ndim_s, figsize=(10, 7), sharex=True)\n",
    "samples_s = sampler_s.get_chain()\n",
    "for i in range(ndim_s):\n",
    "    ax = axes[i]\n",
    "    ax.plot(samples_s[:, :, i], \"k\", alpha=0.3)\n",
    "    ax.set_xlim(0, len(samples_s))\n",
    "    #ax.set_ylim(0, 5)\n",
    "    ax.set_ylabel(labels_s[i])\n",
    "    ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "\n",
    "axes[-1].set_xlabel(\"step number\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the sample autocorrelation time\n",
    "tau_s=sampler_s.get_autocorr_time()\n",
    "print(tau_s)\n",
    "avgtau_s=round(np.average(tau_s))\n",
    "print(avgtau_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discard a few times tau as burn-in and thin by tau/2\n",
    "avgtau_s=200\n",
    "flat_samples_s = sampler_s.get_chain(discard=int(2.*avgtau_s), thin=int(round(avgtau_s/2.)), flat=True)\n",
    "print(flat_samples_s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "fig = corner.corner(flat_samples_s, labels=labels_s, quantiles=[0.16, 0.5, 0.84], show_titles=True, title_fmt='0.3f');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nr_fano] *",
   "language": "python",
   "name": "conda-env-nr_fano-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
